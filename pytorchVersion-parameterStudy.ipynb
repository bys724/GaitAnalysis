{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e0fefc6-2701-4e53-a88b-8887593427b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from scipy.signal import butter, filtfilt, find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51d2bdd9-b292-4e91-863d-1e020081b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Low Pass Filter for Azure data\n",
    "def butter_lowpass_filter(data, cutoff, order):\n",
    "    normal_cutoff=cutoff/(15) \n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    x = np.expand_dims(filtfilt(b, a, data[:,0]), axis=-1)\n",
    "    y = np.expand_dims(filtfilt(b, a, data[:,1]), axis=-1)\n",
    "    z = np.expand_dims(filtfilt(b, a, data[:,2]), axis=-1)\n",
    "    filtered = np.concatenate((x, y, z), axis=-1)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f21e669-14e0-43b1-9401-3d3df393c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadCSV(path):\n",
    "    \n",
    "    csv_data = np.loadtxt(path, delimiter=',', dtype=str, skiprows=1, usecols=(101))\n",
    "    idx_footSwitch = 0\n",
    "    for i in csv_data:\n",
    "        if i == ' 0':\n",
    "            idx_footSwitch += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    ## 누락된 데이터 전까지 읽기 위해 line_num 찾는 과정\n",
    "    csv_data = np.loadtxt(path, delimiter=',', dtype=str, skiprows=1, usecols=(1))\n",
    "    line_num=csv_data.shape[0]\n",
    "    for i, v in enumerate(csv_data):\n",
    "        if v == \" \":\n",
    "            line_num = i\n",
    "            break\n",
    "            \n",
    "    csv_data = np.loadtxt(path, delimiter=',', skiprows=1, max_rows=line_num, usecols=range(1, 101))  \n",
    "    \n",
    "    ## 마지막 4개의 항목을 통해 각 데이터가 기록된 시간을 획득(키넥트 기준)\n",
    "    time = csv_data[:,-4:]\n",
    "    time_ = time[:, 0] * 3600 + time[:,1] * 60 + time[:, 2]+ time[:, 3] / 1000.0\n",
    "    time_ = time_ - time_[0]\n",
    "    initialContactTime = time_[idx_footSwitch]\n",
    "    \n",
    "    ## 포즈 데이터 읽어오기\n",
    "    \n",
    "    # LPF를 위한 변수\n",
    "    cutoff = 6\n",
    "    order = 30\n",
    "    \n",
    "    head_idx = 26\n",
    "    Head = csv_data[:,head_idx * 3:(head_idx + 1) * 3]\n",
    "    Head_filtered = butter_lowpass_filter(Head, cutoff, order)\n",
    "    \n",
    "    pelvis_idx = 0\n",
    "    spine_naval_idx = 1\n",
    "    spine_chest_idx = 2\n",
    "    neck_idx = 3\n",
    "    \n",
    "    Pelvis = csv_data[:,pelvis_idx * 3:(pelvis_idx + 1) * 3]\n",
    "    Spine_naval = csv_data[:,spine_naval_idx * 3:(spine_naval_idx + 1) * 3]\n",
    "    Spine_chest = csv_data[:,spine_chest_idx * 3:(spine_chest_idx + 1) * 3]\n",
    "    Neck = csv_data[:,neck_idx * 3:(neck_idx + 1) * 3]\n",
    "    \n",
    "    Pelvis_filtered = butter_lowpass_filter(Pelvis, cutoff, order)\n",
    "    Spine_naval_filtered = butter_lowpass_filter(Spine_naval, cutoff, order)\n",
    "    Spine_chest_filtered = butter_lowpass_filter(Spine_chest, cutoff, order)\n",
    "    Neck_filtered = butter_lowpass_filter(Neck, cutoff, order)\n",
    "    \n",
    "    \n",
    "    LHip_idx = 18 \n",
    "    LKnee_idx = 19\n",
    "    LAnkle_idx = 20\n",
    "    RHip_idx = 22\n",
    "    RKnee_idx = 23\n",
    "    RAnkle_idx = 24\n",
    "    \n",
    "    LHip = csv_data[:, LHip_idx * 3: (LHip_idx + 1) * 3]\n",
    "    LKnee = csv_data[:, LKnee_idx * 3: (LKnee_idx + 1) * 3]\n",
    "    LAnkle = csv_data[:, LAnkle_idx * 3: (LAnkle_idx + 1) * 3]\n",
    "    \n",
    "    LHip_filtered = butter_lowpass_filter(LHip, cutoff, order)\n",
    "    LKnee_filtered = butter_lowpass_filter(LKnee, cutoff, order)\n",
    "    LAnkle_filtered = butter_lowpass_filter(LAnkle, cutoff, order)\n",
    "    \n",
    "    RHip = csv_data[:, RHip_idx * 3: (RHip_idx + 1) * 3]\n",
    "    RKnee = csv_data[:, RKnee_idx * 3: (RKnee_idx + 1) * 3]\n",
    "    RAnkle = csv_data[:, RAnkle_idx * 3: (RAnkle_idx + 1) * 3]\n",
    "    \n",
    "    RHip_filtered = butter_lowpass_filter(RHip, cutoff, order)\n",
    "    RKnee_filtered = butter_lowpass_filter(RKnee, cutoff, order)\n",
    "    RAnkle_filtered = butter_lowpass_filter(RAnkle, cutoff, order)\n",
    "\n",
    "    ## 발목거리\n",
    "    AngkleDistance = RAnkle_filtered - LAnkle_filtered\n",
    "    AngkleDistance = np.linalg.norm(AngkleDistance, axis=1, keepdims=True)\n",
    "    \n",
    "    ## 무릎각도\n",
    "    knee2hip = LHip_filtered - LKnee_filtered\n",
    "    knee2hip = knee2hip / np.linalg.norm(knee2hip, axis = 1, keepdims=True)\n",
    "    knee2angkle = LAnkle_filtered - LKnee_filtered\n",
    "    knee2angkle = knee2angkle / np.linalg.norm(knee2angkle, axis = 1, keepdims=True)\n",
    "    inner = np.sum(np.multiply(knee2hip, knee2angkle), axis=-1)\n",
    "    angle_Lknee = np.expand_dims(180 - np.arccos(inner) * 180 / np.pi, axis=-1)\n",
    "    \n",
    "    knee2hip = RHip_filtered - RKnee_filtered\n",
    "    knee2hip = knee2hip / np.linalg.norm(knee2hip, axis = 1, keepdims=True)\n",
    "    knee2angkle = RAnkle_filtered - RKnee_filtered\n",
    "    knee2angkle = knee2angkle / np.linalg.norm(knee2angkle, axis = 1, keepdims=True)\n",
    "    inner = np.sum(np.multiply(knee2hip, knee2angkle), axis=-1)\n",
    "    angle_Rknee = np.expand_dims(180 - np.arccos(inner) * 180 / np.pi, axis=-1)\n",
    "    \n",
    "    ## 엉덩이 각도\n",
    "    pelvis2naval = butter_lowpass_filter(Spine_naval, cutoff, order) - butter_lowpass_filter(Pelvis, cutoff, order)\n",
    "    pelvis2naval = pelvis2naval / np.linalg.norm(pelvis2naval, axis = 1, keepdims=True)\n",
    "    hip2knee = LKnee_filtered - LHip_filtered\n",
    "    hip2knee = hip2knee / np.linalg.norm(hip2knee, axis = 1, keepdims=True)\n",
    "    inner = np.sum(np.multiply(pelvis2naval, hip2knee), axis=-1)\n",
    "    angle_LHip = np.expand_dims(180 - np.arccos(inner) * 180 / np.pi, axis=-1)\n",
    "    \n",
    "    hip2knee = RKnee_filtered - RHip_filtered\n",
    "    hip2knee = hip2knee / np.linalg.norm(hip2knee, axis = 1, keepdims=True)\n",
    "    inner = np.sum(np.multiply(pelvis2naval, hip2knee), axis=-1)\n",
    "    angle_RHip = np.expand_dims(180 - np.arccos(inner) * 180 / np.pi, axis=-1)\n",
    "    \n",
    "    \n",
    "    ## pelvis to ankle\n",
    "    pevis2Lankle = Pelvis_filtered - LAnkle_filtered\n",
    "    pevis2Rankle = Pelvis_filtered - RAnkle_filtered\n",
    "    \n",
    "    # Safe Zone 설정\n",
    "    # Pelvis의 z방향 성분을 통해 어떤 프레임부터 어떤 프레임까지 사용할지 결정\n",
    "    # 카메라에서 먼 지점이 safe_start, 카메라에서 가까운 지점이 safe_end 이다.(피실험자는 카메라에서 먼 곳에서부터 카메라를 향해 보행함) \n",
    "    safe_start=0\n",
    "    safe_end=0\n",
    "    \n",
    "    # 현재 safe_start는 pelvis의 z방향 성분이 3300mm 이하로 떨어지는 지점, safe_end는 1350 이하로 떨어지는 지점\n",
    "    for i, v in enumerate(Pelvis[:, 2]):\n",
    "        if safe_start == 0 and v < 3300:\n",
    "            safe_start = i\n",
    "        if safe_start != 0 and v < 1350:\n",
    "            safe_end = i\n",
    "            break\n",
    "    # Head_z\n",
    "    Head_filtered_z = np.expand_dims(Head_filtered[:, -1], axis = -1)\n",
    "    \n",
    "    # data1은 왼다리, data2는 오른다리에 대한 Input(엉덩이각도, 무릎각도, 반대쪽 엉덩이각도, 반대쪽 무릎각도, 양 발목간의 거리)\n",
    "    # Output : 후처리 결과(true or false), data1, data2, initialContactTime(키넥트 시스템 기준), safe_start(프레임 넘버), safe_end(프레임 넘버)\n",
    "    data_l = None\n",
    "    data_r = None\n",
    "    \n",
    "    if inputType == 0:\n",
    "        data_l = np.concatenate((angle_LHip, angle_Lknee, angle_RHip, angle_Rknee, AngkleDistance), axis=-1)  \n",
    "        data_r = np.concatenate((angle_RHip, angle_Rknee, angle_LHip, angle_Lknee, AngkleDistance), axis=-1) \n",
    "    elif inputType == 1:\n",
    "        data_l = np.concatenate((angle_LHip, angle_Lknee, AngkleDistance), axis=-1)  \n",
    "        data_r = np.concatenate((angle_RHip, angle_Rknee, AngkleDistance), axis=-1) \n",
    "    elif inputType == 2:\n",
    "        data_l = np.concatenate((angle_Lknee, angle_Rknee, AngkleDistance), axis=-1)  \n",
    "        data_r = np.concatenate((angle_Rknee, angle_Lknee, AngkleDistance), axis=-1) \n",
    "    elif inputType == 3:\n",
    "        data_l = np.concatenate((LHip_filtered, LKnee_filtered, LAnkle_filtered, RHip_filtered, RKnee_filtered, RAnkle_filtered), axis=-1)  \n",
    "        data_r = np.concatenate((RHip_filtered, RKnee_filtered, RAnkle_filtered, LHip_filtered, LKnee_filtered, LAnkle_filtered), axis=-1)  \n",
    "    elif inputType == 4:\n",
    "        data_l = np.concatenate((angle_LHip, angle_Lknee, angle_RHip, angle_Rknee), axis=-1)  \n",
    "        data_r = np.concatenate((angle_RHip, angle_Rknee, angle_LHip, angle_Lknee), axis=-1) \n",
    "    elif inputType == 5:\n",
    "        data_l = np.concatenate((angle_LHip, angle_Lknee, angle_RHip, angle_Rknee, AngkleDistance, Head_filtered_z), axis=-1)  \n",
    "        data_r = np.concatenate((angle_RHip, angle_Rknee, angle_LHip, angle_Lknee, AngkleDistance, Head_filtered_z), axis=-1) \n",
    "    \n",
    "    return data_l, data_r, time_, initialContactTime, safe_start, safe_end, pevis2Lankle, pevis2Rankle\n",
    "\n",
    "#inputType = 4\n",
    "#data_l, data_r, time_, initialContactTime, safe_start, safe_end, pevis2Lankle, pevis2Rankle = ReadCSV('Dataset/BYS/BYS_00.csv')\n",
    "#print(data_l.shape)\n",
    "#0 : (angle_RHip, angle_Rknee, angle_LHip, angle_Lknee, AngkleDistance)\n",
    "#1 : (angle_RHip, angle_Rknee, AngkleDistance)\n",
    "#2 : (angle_Rknee, angle_Lknee, AngkleDistance)\n",
    "#3 : (RHip, RKnee, RAnkle, LHip, LKnee, LAnkle)\n",
    "#4 : (angle_RHip, angle_Rknee, angle_LHip, angle_Lknee)\n",
    "#5 : (angle_RHip, angle_Rknee, angle_LHip, angle_Lknee, AngkleDistance, Head_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8612aa1-fcfa-4960-83ed-763bbe9f0765",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zebris에서 제공하는 pressure 데이터를 읽기 위한 함수\n",
    "# file은 각 trail 마다의 폴더경로\n",
    "def ReadForceData(path):\n",
    "    \n",
    "    # Parameters\n",
    "    path_parameters = os.path.join(path, \"parameters.csv\")\n",
    "    df = pd.read_csv(path_parameters)\n",
    "    df_extracted = df[['Step length L, cm', 'Stance phase L, %', 'Step length R, cm', 'Stance phase R, %', 'Stride length, cm', 'Cadence, steps/min', 'Velocity, km/h']]\n",
    "    #print(df_extracted.values.tolist()[0])\n",
    "    \n",
    "    # 각 폴더마다 양 발에 대한 bufferfly_force_curve 파일이 존재\n",
    "    path_l = os.path.join(path, \"butterfly_force_curve-L.csv\")\n",
    "    path_r = os.path.join(path, \"butterfly_force_curve-R.csv\")\n",
    "    \n",
    "    # 왼발에 대한 initial contact과 Toe off 지점을 찾는 파트\n",
    "    csv_data_l = np.loadtxt(path_l, delimiter=',', skiprows=4, usecols=range(2), encoding='utf-8')\n",
    "    Ltime = csv_data_l[:,0]\n",
    "    Lforce = csv_data_l[:,1]\n",
    "\n",
    "    l_contact = []\n",
    "    l_off = []\n",
    "    l_contact.append(Ltime[0])\n",
    "    pre = Ltime[0]\n",
    "    for i in Ltime:\n",
    "        if i - pre > 0.2:\n",
    "            l_contact.append(i)\n",
    "            if pre != 0:\n",
    "                l_off.append(pre + 0.01)\n",
    "        pre = i\n",
    "    l_off.append(Ltime[-1] + 0.01)\n",
    "    \n",
    "    # 오른발에 대한 initial contact과 Toe off 지점을 찾는 파트\n",
    "    csv_data_r = np.loadtxt(path_r, delimiter=',', skiprows=4, usecols=range(2), encoding='utf-8')\n",
    "    Rtime = csv_data_r[:,0]\n",
    "    Rforce = csv_data_r[:,1]\n",
    "\n",
    "    r_contact = []\n",
    "    r_off = []\n",
    "    r_contact.append(Rtime[0])\n",
    "    pre = Rtime[0]\n",
    "    for i in Rtime:\n",
    "        if i - pre > 0.2:\n",
    "            r_contact.append(i)\n",
    "            if pre != 0:\n",
    "                r_off.append(pre + 0.01)\n",
    "        pre = i\n",
    "    r_off.append(Rtime[-1] + 0.01)\n",
    "    \n",
    "    # Output : 후처리 결과(true or false), l_contact(시간 list), l_off(시간 list), r_contact(시간 list), r_off(시간 list)\n",
    "    return l_contact, l_off, r_contact, r_off, df_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "560374fd-e169-4a74-8b8e-04100bfde40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, numFrame, drawing):\n",
    "        global inputDims\n",
    "        self.root = root\n",
    "        self.numFrame = numFrame\n",
    "        names = []\n",
    "        for name in next(os.walk(root))[1]:\n",
    "            names.append(name)\n",
    "        #print(names)\n",
    "        self.names = names\n",
    "        self.errorFiles = []\n",
    "        \n",
    "        #drawing을 위한 폴더 생성\n",
    "        if drawing:\n",
    "            if not os.path.isdir(\"./Inputs\"):\n",
    "                os.mkdir(\"Inputs\")\n",
    "            else:\n",
    "                shutil.rmtree(\"Inputs\")\n",
    "                os.mkdir(\"Inputs\")\n",
    "            \n",
    "        \n",
    "        # 인공지능을 학습하기 위한 모든 input과 output을 저장하기 위한 버퍼\n",
    "        data_l_test, data_r_test, time_, initialContactTime, safe_start, safe_end, pevis2Lankle, pevis2Rankle = ReadCSV('Dataset/BYS/BYS_00.csv')\n",
    "        inputDims = data_l_test.shape[-1]\n",
    "\n",
    "        inputs_buffer = np.zeros(shape=(1, numFrame, inputDims), dtype=np.float64)\n",
    "        outputs_buffer = np.zeros(shape=(1, ), dtype=np.float64)\n",
    "        \n",
    "        for name in self.names:\n",
    "            pathPerson = os.path.join(self.root, name)\n",
    "            print(pathPerson)\n",
    "            \n",
    "            (root, Folders, csvFiles) = next(os.walk(pathPerson))\n",
    "            Folders = sorted(Folders)\n",
    "            csvFiles = sorted(csvFiles)\n",
    "            \n",
    "            if '.ipynb_checkpoints' in Folders:\n",
    "                Folders.remove('.ipynb_checkpoints')\n",
    "            if '.ipynb_checkpoints' in csvFiles:\n",
    "                csvFiles.remove('.ipynb_checkpoints')\n",
    "            if len(Folders) != len(csvFiles):\n",
    "                print(name, ' is not matched')\n",
    "                break\n",
    "            \n",
    "            for idx_file in range(len(csvFiles)):\n",
    "                pathCSV = os.path.join(pathPerson, csvFiles[idx_file])\n",
    "                pathForceData = os.path.join(pathPerson, Folders[idx_file])\n",
    "                \n",
    "                #try:\n",
    "                data_l, data_r, time, initialContactTime, safe_start, safe_end, pevis2Lankle, pevis2Rankle = ReadCSV(pathCSV)\n",
    "                l_contact, l_off, r_contact, r_off, parameters = ReadForceData(pathForceData)\n",
    "\n",
    "                # 싱크 맞추기\n",
    "                firstContact = min(r_contact[0], l_contact[0]) # zebris 기준 최초 contact 찾음\n",
    "                timeGap = initialContactTime - firstContact # sync.kinect 기준으로 모든 시간 변환\n",
    "                l_contact = l_contact + timeGap\n",
    "                l_off = l_off + timeGap\n",
    "                r_contact = r_contact + timeGap\n",
    "                r_off = r_off + timeGap\n",
    "\n",
    "                # 정답 라벨\n",
    "                l_contact_ = []\n",
    "                for i in l_contact:\n",
    "                    tmp = abs(time - i)\n",
    "                    l_contact_.append(np.argmin(tmp))\n",
    "                l_off_ = []\n",
    "                for i in l_off:\n",
    "                    tmp = abs(time - i)\n",
    "                    l_off_.append(np.argmin(tmp))\n",
    "\n",
    "                l_label = np.zeros(data_l.shape[0])\n",
    "                for i in range(len(l_contact_)):\n",
    "                    for j in range(l_contact_[i], l_off_[i]):\n",
    "                        l_label[j] = 1\n",
    "\n",
    "                r_contact_ = []\n",
    "                for i in r_contact:\n",
    "                    tmp = abs(time - i)\n",
    "                    r_contact_.append(np.argmin(tmp))\n",
    "                r_off_ = []\n",
    "                for i in r_off:\n",
    "                    tmp = abs(time - i)\n",
    "                    r_off_.append(np.argmin(tmp))\n",
    "\n",
    "                r_label = np.zeros(data_r.shape[0])\n",
    "                for i in range(len(r_contact_)):\n",
    "                    for j in range(r_contact_[i], r_off_[i]):\n",
    "                        r_label[j] = 1\n",
    "\n",
    "                # 인공지능 인풋을 만들기 위한 루프\n",
    "                # safe_start와 safe_end 사이에서 numFrame(현재 10)개씩 잘라서 input 하나씩 생성\n",
    "                # input에 대한 결과는 l_label 또는 r_label을 이용해 파악\n",
    "                idx_startframe = safe_start\n",
    "                while (idx_startframe + numFrame-1) <= safe_end:\n",
    "                    ## L\n",
    "                    input_ = data_l[idx_startframe:idx_startframe+numFrame, :]\n",
    "                    min_ = np.min(input_,axis=0)\n",
    "                    max_ = np.max(input_,axis=0)\n",
    "                    input_ = (input_ - min_) / (max_ - min_)\n",
    "                    input_ = np.expand_dims(input_, axis=0)\n",
    "                    inputs_buffer = np.concatenate((inputs_buffer, input_), axis=0)\n",
    "\n",
    "                    output_ = l_label[idx_startframe+numFrame-1]\n",
    "                    outputs_buffer = np.concatenate((outputs_buffer, np.array([output_])), axis=0)\n",
    "                    #output_ = np.expand_dims(output_, axis=0)\n",
    "                    #outputs_buffer = np.concatenate((outputs_buffer, output_), axis=0)\n",
    "\n",
    "                    ## R\n",
    "                    input_ = data_r[idx_startframe:idx_startframe+numFrame, :]\n",
    "                    min_ = np.min(input_,axis=0)\n",
    "                    max_ = np.max(input_,axis=0)\n",
    "                    input_ = (input_ - min_) / (max_ - min_)\n",
    "                    input_ = np.expand_dims(input_, axis=0)\n",
    "                    inputs_buffer = np.concatenate((inputs_buffer, input_), axis=0)\n",
    "\n",
    "                    output_ = r_label[idx_startframe+numFrame-1]\n",
    "                    outputs_buffer = np.concatenate((outputs_buffer, np.array([output_])), axis=0)\n",
    "                    #output_ = np.expand_dims(output_, axis=0)\n",
    "                    #outputs_buffer = np.concatenate((outputs_buffer, output_), axis=0)\n",
    "                    idx_startframe += 1\n",
    "\n",
    "                # Drawing\n",
    "                if drawing:\n",
    "                    x_new = np.linspace(time[0], time[-1], len(time))\n",
    "                    fig, axs = plt.subplots(4, figsize=(18,12))\n",
    "\n",
    "                    ## Left Input\n",
    "                    axs[0].set_title('Left')\n",
    "                    axs[0].plot(x_new, data_l[:,0], 'r-', label = 'Hip')\n",
    "                    axs[0].plot(x_new, data_l[:,1], 'b-', label = 'Knee')\n",
    "                    axs[0].plot(x_new, data_l[:,4], 'k-', label = 'AnkleDistance')\n",
    "\n",
    "                    axs[0].axvline(x = time[l_contact_[0]], label='LContact', c='r')\n",
    "                    for i in l_contact_:\n",
    "                        axs[0].axvline(x = time[i], c='r')\n",
    "                    axs[0].axvline(x = time[l_off_[0]], label='LOff', c='b')\n",
    "                    for i in l_off_:\n",
    "                        axs[0].axvline(x = time[i], c='b')\n",
    "                    axs[0].axvline(x = time[safe_start], label='safe Start = {}'.format(safe_start), c='g')\n",
    "                    axs[0].axvline(x = time[safe_end], label='safe End = {}'.format(safe_end), c='g')\n",
    "                    axs[0].legend()\n",
    "\n",
    "                    ## Left Label\n",
    "                    axs[1].set_title('Left_label')\n",
    "\n",
    "                    for i, v in enumerate(l_label):\n",
    "                        if v == 1:\n",
    "                            if i == 0:\n",
    "                                axs[1].plot(time[i], 0.7, 'b.', label='Swing')\n",
    "                                axs[1].plot(time[i], 0.7, 'r.', label='Stance')\n",
    "                            else:\n",
    "                                axs[1].plot(time[i], 0.7, 'r.')\n",
    "                        else:\n",
    "                            if i == 0:\n",
    "                                axs[1].plot(time[i], 0.3, 'r.', label='Stance')\n",
    "                                axs[1].plot(time[i], 0.3, 'b.', label='Swing')\n",
    "                            else:\n",
    "                                axs[1].plot(time[i], 0.3, 'b.')\n",
    "                    axs[1].axvline(x = time[safe_start], c='g')\n",
    "                    axs[1].axvline(x = time[safe_end], c='g')\n",
    "                    axs[1].legend()\n",
    "\n",
    "                    ## Right Input\n",
    "                    axs[2].set_title('Right')\n",
    "                    axs[2].plot(x_new, data_r[:,0], 'r-', label = 'Hip')\n",
    "                    axs[2].plot(x_new, data_r[:,1], 'b-', label = 'Knee')\n",
    "                    axs[2].plot(x_new, data_r[:,4], 'k-', label = 'AnkleDistance')\n",
    "                    axs[2].axvline(x = time[r_contact_[0]], label='RContact', c='r')\n",
    "                    for i in r_contact_:\n",
    "                        axs[2].axvline(x = time[i], c='r')\n",
    "                    axs[2].axvline(x = time[r_off_[0]], label='ROff', c='b')\n",
    "                    for i in r_off_:\n",
    "                        axs[2].axvline(x = time[i], c='b')\n",
    "                    axs[2].axvline(x = time[safe_start], label='safe Start = {}'.format(safe_start), c='g')\n",
    "                    axs[2].axvline(x = time[safe_end], label='safe End = {}'.format(safe_end), c='g')\n",
    "                    axs[2].legend()\n",
    "\n",
    "                    ## Right Label\n",
    "                    axs[3].set_title('Right_label')\n",
    "                    for i, v in enumerate(r_label):\n",
    "                        if v == 1:\n",
    "                            if i == 0:\n",
    "                                axs[3].plot(time[i], 0.7, 'b.', label='Swing')\n",
    "                                axs[3].plot(time[i], 0.7, 'r.', label='Stance')\n",
    "                            else:\n",
    "                                axs[3].plot(time[i], 0.7, 'r.')\n",
    "                        else:\n",
    "                            if i == 0:\n",
    "                                axs[3].plot(time[i], 0.3, 'r.', label='Stance')\n",
    "                                axs[3].plot(time[i], 0.3, 'b.', label='Swing')\n",
    "                            else:\n",
    "                                axs[3].plot(time[i], 0.3, 'b.')\n",
    "                    axs[3].axvline(x = time[safe_start], c='g')\n",
    "                    axs[3].axvline(x = time[safe_end], c='g')\n",
    "                    axs[3].legend()\n",
    "\n",
    "\n",
    "                    for ax in axs:\n",
    "                        ax.set_xlim([x_new[0], x_new[-1]])\n",
    "\n",
    "                    fig.tight_layout()\n",
    "                    plt.savefig('Inputs//' + csvFiles[idx_file].split('.')[0] + '.jpg')\n",
    "                    plt.close()\n",
    "                    \n",
    "                #except:\n",
    "                #    self.errorFiles.append(csvFiles[idx_file].split('.')[0])\n",
    "                \n",
    "                #AzureData(pathCSV)\n",
    "        inputs_buffer = np.delete(inputs_buffer, 0, axis=0)\n",
    "        outputs_buffer = np.delete(outputs_buffer, 0, axis=0)\n",
    "        #print(inputs_buffer.shape)\n",
    "        #self.x_data = np.swapaxes(inputs_buffer, 1, 2)\n",
    "        self.x_data = inputs_buffer\n",
    "        print('input shape : ', self.x_data.shape)\n",
    "        self.y_data = outputs_buffer\n",
    "        print('output shape : ', self.y_data.shape)\n",
    "        print(\"Finish !\")\n",
    "        if len(self.errorFiles) > 0:\n",
    "            print(self.errorFiles)\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.x_data[idx,:,:])\n",
    "        y = torch.FloatTensor([self.y_data[idx]])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92c31a3b-e1b8-4e0c-962c-16ffa1532437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ruleBased(numFrame, safe_start, safe_end, pevis2Lankle, pevis2Rankle, PostProcessing_):\n",
    "    # Safe Zone안에 데이터만 다룸\n",
    "    pevis2Lankle = pevis2Lankle[safe_start + numFrame - 1:safe_end + 1, -1]\n",
    "    pevis2Rankle = pevis2Rankle[safe_start + numFrame - 1:safe_end + 1, -1]\n",
    "    \n",
    "    # For Left\n",
    "    LContact, _ = find_peaks(pevis2Lankle,  distance=3)\n",
    "    LOff, _ = find_peaks(-pevis2Lankle,  distance=3)\n",
    "    \n",
    "    tmp = 0\n",
    "    if LContact[0] > LOff[0]:\n",
    "        tmp = 1\n",
    "    l_answer = []\n",
    "    for i in range(pevis2Lankle.shape[0]):\n",
    "        if i in LContact:\n",
    "            tmp = 1\n",
    "        elif i in LOff:\n",
    "            tmp = 0\n",
    "        l_answer.append(tmp)\n",
    "        \n",
    "    # For Right\n",
    "    RContact, _ = find_peaks(pevis2Rankle, distance=3)\n",
    "    ROff, _ = find_peaks(-pevis2Rankle, distance=3)\n",
    "    \n",
    "    tmp = 0\n",
    "    if RContact[0] > ROff[0]:\n",
    "        tmp = 1\n",
    "    r_answer = []\n",
    "    for i in range(pevis2Rankle.shape[0]):\n",
    "        if i in RContact:\n",
    "            tmp = 1\n",
    "        elif i in ROff:\n",
    "            tmp = 0\n",
    "        r_answer.append(tmp)\n",
    "        \n",
    "    finalAnswer_l = None\n",
    "    finalAnswer_r = None\n",
    "    \n",
    "    if PostProcessing_:\n",
    "        finalAnswer_l = PostProcessing(answer = l_answer, threshold = 3)\n",
    "        finalAnswer_r = PostProcessing(answer = r_answer, threshold = 3)\n",
    "    else:\n",
    "        finalAnswer_l = l_answer\n",
    "        finalAnswer_r = r_answer\n",
    "        \n",
    "        \n",
    "    return finalAnswer_l, finalAnswer_r, l_answer, r_answer\n",
    "            \n",
    "#finalAnswer_l, finalAnswer_r = ruleBased(numFrame = 10, path = './Dataset/BYS/BYS_00.csv')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30fdab07-0925-43fd-9e5c-c512a016bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PostProcessing(answer, threshold):\n",
    "    # L\n",
    "    # count : 연속적인 gait Phase를 세기 위한 변수\n",
    "    # prev : 바로 직전의 gait phase\n",
    "    # mark_unstable : 불연속적인 구간기록\n",
    "    count = 0\n",
    "    prev = answer[0]\n",
    "    mark_unstable = []\n",
    "    for i, answer_ in enumerate(answer):\n",
    "        # Gait Phase가 바뀌는 시점\n",
    "        if prev != answer_:\n",
    "            # count가 정해준 threshold를 넘어갔을 때, 즉 연속적인 구간일 때 0\n",
    "            if count > threshold:\n",
    "                mark_unstable += [0] * count\n",
    "            # count가 정해준 threshold보다 작을 때, 즉 불연속적인 구간일 때 -1\n",
    "            else:\n",
    "                mark_unstable += [-1] * count\n",
    "            count = 1\n",
    "        # Gait Phase가 바뀌지 않는 시점\n",
    "        # count 만 올라감\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "        # Gait answer 끝부분에 대한 처리구간\n",
    "        if i == len(answer) - 1:\n",
    "            # 끝부분에 다다랐을 때 count가 threshold를 넘어갔을 때, 즉 연속적인 구간일 때 0\n",
    "            if count > threshold:\n",
    "                mark_unstable += [0] * count\n",
    "            # 끝부분에 다다랐을 때 count가 threshold보다 작을 때, 즉 불연속적인 구간일 때 -1\n",
    "            else:\n",
    "                mark_unstable += [-1] * count\n",
    "\n",
    "        prev = answer_\n",
    "    #print(mark_unstable)\n",
    "\n",
    "    \n",
    "    # 위에서 기록한 mark_unstable를 근거로 range_unstable 생성\n",
    "    # 생성된 range_unstable은 불연속적인 구간들의 처음과 끝을 묶은 리스트들로 구성\n",
    "    # 예) [[1, 3], [8, 13], ...]\n",
    "    unstable = False\n",
    "    range_unstable = []\n",
    "    tmp = []\n",
    "    for i, v in enumerate(mark_unstable):\n",
    "        # tmp가 하나의 불연속적인 구간을 나타냄\n",
    "        if v == -1 and not unstable:\n",
    "            tmp.append(i)\n",
    "            unstable = True\n",
    "        elif v != -1 and unstable:\n",
    "            tmp.append(i)\n",
    "            range_unstable.append(tmp)\n",
    "            tmp = []\n",
    "            unstable = False\n",
    "        elif v == -1 and i == len(mark_unstable)-1:\n",
    "            tmp.append(i)\n",
    "            range_unstable.append(tmp)\n",
    "            tmp = []\n",
    "            unstable = False\n",
    "    #print(range_unstable)\n",
    "\n",
    "    filtered_answer = answer\n",
    "\n",
    "    # 생성된 range_unstable를 기반으로 filtering하는 부분\n",
    "    # 각각의 불연속적인 구간에 대해서 양 옆의 gait phase를 확인하고, 그에 맞춰서 불연속적인 구간에 대한 정답보정 \n",
    "    for range_ in range_unstable:\n",
    "\n",
    "        left_class = None\n",
    "        if range_[0]-1 < 0:\n",
    "            left_class = 1 - filtered_answer[range_[1]+1]\n",
    "            #left_class = filtered_answer[range_[1]+1]\n",
    "        else:\n",
    "            left_class = filtered_answer[range_[0]-1]\n",
    "\n",
    "        right_class = None\n",
    "        if range_[1]+1 == len(answer):\n",
    "            right_class = 1 - filtered_answer[range_[0]-1]\n",
    "            #right_class = filtered_answer[range_[0]-1]\n",
    "        else:\n",
    "            right_class = filtered_answer[range_[1] + 1]\n",
    "\n",
    "        for i in range(range_[0], int(sum(range_) / 2)):\n",
    "            filtered_answer[i] = left_class\n",
    "\n",
    "        for i in range(int(sum(range_) / 2), range_[1] + 1):\n",
    "            filtered_answer[i] = right_class\n",
    "    #print(filtered_answer)\n",
    "    return filtered_answer\n",
    "\n",
    "def ModelTest(numFrame, pathCSV, model, PostProcessing_):\n",
    "    data_l, data_r, time, initialContactTime, safe_start, safe_end, pevis2Lankle, pevis2Rankle = ReadCSV(pathCSV)\n",
    "    \n",
    "    l_answer = []\n",
    "    r_answer = []\n",
    "\n",
    "    idx_startframe = safe_start\n",
    "    while (idx_startframe + numFrame-1) <= safe_end:\n",
    "        ## L\n",
    "        input_ = data_l[idx_startframe:idx_startframe+numFrame, :]\n",
    "        min_ = np.min(input_,axis=0)\n",
    "        max_ = np.max(input_,axis=0)\n",
    "        input_ = (input_ - min_) / (max_ - min_)\n",
    "        input_ = np.expand_dims(input_, axis=0)\n",
    "        input_ = torch.FloatTensor(input_).to(device)\n",
    "        h0 = torch.zeros(1, 1, hidden_size).to(device)\n",
    "        c0 = torch.zeros(1, 1, hidden_size).to(device)\n",
    "        pred = model(input_, h0.detach(), c0.detach())\n",
    "        answer = 0\n",
    "        if torch.squeeze(pred) > 0.5:\n",
    "            answer = 1\n",
    "        else:\n",
    "            answer = 0\n",
    "        l_answer.append(answer)\n",
    "\n",
    "        ## R\n",
    "        input_ = data_r[idx_startframe:idx_startframe+numFrame, :]\n",
    "        min_ = np.min(input_,axis=0)\n",
    "        max_ = np.max(input_,axis=0)\n",
    "        input_ = (input_ - min_) / (max_ - min_)\n",
    "        input_ = np.expand_dims(input_, axis=0)\n",
    "        input_ = torch.FloatTensor(input_).to(device)\n",
    "        h0 = torch.zeros(1, 1, hidden_size).to(device)\n",
    "        c0 = torch.zeros(1, 1, hidden_size).to(device)\n",
    "        pred = model(input_, h0.detach(), c0.detach())\n",
    "        answer = 0\n",
    "        if torch.squeeze(pred) > 0.5:\n",
    "            answer = 1\n",
    "        else:\n",
    "            answer = 0\n",
    "        r_answer.append(answer)\n",
    "        idx_startframe += 1\n",
    "\n",
    "    finalAnswer_l = None\n",
    "    finalAnswer_r = None\n",
    "    \n",
    "    if PostProcessing_:\n",
    "        finalAnswer_l = PostProcessing(answer = l_answer, threshold = 3)\n",
    "        finalAnswer_r = PostProcessing(answer = r_answer, threshold = 3)\n",
    "    else:\n",
    "        finalAnswer_l = l_answer\n",
    "        finalAnswer_r = r_answer\n",
    "        \n",
    "    return finalAnswer_l, finalAnswer_r, l_answer, r_answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4963fabc-b707-4e43-bcf5-4c86217fb816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictParamters(path, l_contact_predict, l_off_predict, r_contact_predict, r_off_predict, safe_start):\n",
    "    \n",
    "    ## 누락된 데이터 전까지 읽기 위해 line_num 찾는 과정\n",
    "    csv_data = np.loadtxt(path, delimiter=',', dtype=str, skiprows=1, usecols=(1))\n",
    "    line_num=csv_data.shape[0]\n",
    "    for i, v in enumerate(csv_data):\n",
    "        if v == \" \":\n",
    "            line_num = i\n",
    "            break\n",
    "            \n",
    "    csv_data = np.loadtxt(path, delimiter=',', skiprows=1, max_rows=line_num, usecols=range(1, 101))  \n",
    "    \n",
    "    ## 마지막 4개의 항목을 통해 각 데이터가 기록된 시간을 획득(키넥트 기준)\n",
    "    time = csv_data[:,-4:]\n",
    "    time_ = time[:, 0] * 3600 + time[:,1] * 60 + time[:, 2]+ time[:, 3] / 1000.0\n",
    "    time_ = time_ - time_[0]\n",
    "    \n",
    "    ## 발목 데이터 읽어오기\n",
    "    LAnkle_idx = 20\n",
    "    RAnkle_idx = 24\n",
    "    \n",
    "    LAnkle = csv_data[:, LAnkle_idx * 3: (LAnkle_idx + 1) * 3]\n",
    "    RAnkle = csv_data[:, RAnkle_idx * 3: (RAnkle_idx + 1) * 3]\n",
    "        \n",
    "    # LPF를 위한 변수\n",
    "    cutoff = 6\n",
    "    order = 30\n",
    "    LAnkle_filtered = butter_lowpass_filter(LAnkle, cutoff, order)\n",
    "    RAnkle_filtered = butter_lowpass_filter(RAnkle, cutoff, order)\n",
    "    \n",
    "    # strideLength\n",
    "    count = 0\n",
    "    LstrideLength = 0\n",
    "    if len(l_contact_predict) > 1:\n",
    "        for i in range(len(l_contact_predict)-1, 0, -1):\n",
    "            #LstrideLength += np.linalg.norm(LAnkle_filtered[l_contact_predict[i]+safe_start] - LAnkle_filtered[l_contact_predict[i-1]+safe_start])\n",
    "            LstrideLength += abs(LAnkle_filtered[l_contact_predict[i]+safe_start, -1] - LAnkle_filtered[l_contact_predict[i-1]+safe_start, -1])\n",
    "            count += 1\n",
    "        LstrideLength /= count\n",
    "        LstrideLength /= 10 # mm ->cm\n",
    "    else:\n",
    "        LstrideLength = None\n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    RstrideLength = 0\n",
    "    if len(r_contact_predict) > 1:\n",
    "        for i in range(len(r_contact_predict)-1, 0, -1):\n",
    "            #RstrideLength += np.linalg.norm(RAnkle_filtered[r_contact_predict[i]+safe_start] - RAnkle_filtered[r_contact_predict[i-1]+safe_start])\n",
    "            RstrideLength += abs(RAnkle_filtered[r_contact_predict[i]+safe_start, -1] - RAnkle_filtered[r_contact_predict[i-1]+safe_start, -1])\n",
    "            count += 1\n",
    "        RstrideLength /= count\n",
    "        RstrideLength /= 10\n",
    "    else:\n",
    "        RstrideLength = None\n",
    "        \n",
    "    # stepLength\n",
    "    stepLength_L = 0\n",
    "    stepLength_R = 0\n",
    "    if len(l_contact_predict) > 0 and len(r_contact_predict) > 0:\n",
    "        count = 0\n",
    "        for i in range(len(l_contact_predict)):\n",
    "            for j in range(len(r_contact_predict)):\n",
    "                if r_contact_predict[j] > l_contact_predict[i]:\n",
    "                    #stepLength_R += np.linalg.norm(RAnkle_filtered[r_contact_predict[j]+safe_start] - LAnkle_filtered[l_contact_predict[i]+safe_start])\n",
    "                    stepLength_R += abs(RAnkle_filtered[r_contact_predict[j]+safe_start, -1] - LAnkle_filtered[l_contact_predict[i]+safe_start, -1])\n",
    "                    count += 1\n",
    "                    break\n",
    "        if count != 0:\n",
    "            stepLength_R /= count\n",
    "            stepLength_R /= 10\n",
    "        else:\n",
    "            stepLength_R = None\n",
    "                    \n",
    "        count = 0\n",
    "        for i in range(len(r_contact_predict)):\n",
    "            for j in range(len(l_contact_predict)):\n",
    "                if l_contact_predict[j] > r_contact_predict[i]:\n",
    "                    #stepLength_L += np.linalg.norm(LAnkle_filtered[l_contact_predict[j]+safe_start] - RAnkle_filtered[r_contact_predict[i]+safe_start])\n",
    "                    stepLength_L += abs(LAnkle_filtered[l_contact_predict[j]+safe_start, -1] - RAnkle_filtered[r_contact_predict[i]+safe_start, -1])\n",
    "                    count += 1\n",
    "                    break\n",
    "        if count != 0:\n",
    "            stepLength_L /= count\n",
    "            stepLength_L /= 10\n",
    "        else:\n",
    "            stepLength_L = None\n",
    "    else:\n",
    "        stepLength_L = None\n",
    "        stepLength_R = None\n",
    "    \n",
    "        \n",
    "    \n",
    "    return LstrideLength, RstrideLength, stepLength_L, stepLength_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "24e9a393-4d36-43a2-807d-80a12ee34835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResults(root, numFrame, modelBased, drawing):\n",
    "    global stopped_epoch\n",
    "    \n",
    "    saveRoot = None\n",
    "    if modelBased:\n",
    "        saveRoot = 'Type' + str(inputType) + '_' + root + '_Result'\n",
    "    else:\n",
    "        saveRoot = 'ruleBased_' + root + '_Result'\n",
    "    \n",
    "    \n",
    "    totalFrame = 0\n",
    "    classification_positive = 0\n",
    "    numTrial = 0\n",
    "    \n",
    "    count_contact = 0\n",
    "    count_off = 0\n",
    "    \n",
    "    FrameError_contact = []\n",
    "    FrameError_off = []\n",
    "    coverage_positive_contact = 0\n",
    "    coverage_positive_off = 0\n",
    "    coverage_negative_contact_list = []\n",
    "    coverage_negative_off_list = []\n",
    "    \n",
    "    Error_strideLength_L = []\n",
    "    Error_strideLength_R = []\n",
    "    Error_stepLength_L = []\n",
    "    Error_stepLength_R = []\n",
    "    \n",
    "    count_strideLength_L = 0\n",
    "    count_strideLength_R = 0\n",
    "    count_stepLength_L = 0\n",
    "    count_stepLength_R = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    names = []\n",
    "    for name in next(os.walk(root))[1]:\n",
    "        names.append(name)\n",
    "    #print(names)\n",
    "    errorFiles = []\n",
    "\n",
    "    #drawing을 위한 폴더 생성\n",
    "    if not os.path.isdir(saveRoot):\n",
    "            os.mkdir(saveRoot)\n",
    "    else:\n",
    "        if drawing:\n",
    "            shutil.rmtree(saveRoot)\n",
    "            os.mkdir(saveRoot)\n",
    "    \n",
    "            \n",
    "    for name in names:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        pathPerson = os.path.join(root, name)\n",
    "        (root_, Folders, csvFiles) = next(os.walk(pathPerson))\n",
    "        Folders = sorted(Folders)\n",
    "        csvFiles = sorted(csvFiles)\n",
    "\n",
    "        if '.ipynb_checkpoints' in Folders:\n",
    "            Folders.remove('.ipynb_checkpoints')\n",
    "        if '.ipynb_checkpoints' in csvFiles:\n",
    "            csvFiles.remove('.ipynb_checkpoints')\n",
    "        if len(Folders) != len(csvFiles):\n",
    "            print(name, ' is not matched')\n",
    "            break\n",
    "\n",
    "        for idx_file in range(len(csvFiles)):\n",
    "            pathCSV = os.path.join(pathPerson, csvFiles[idx_file])\n",
    "            pathForceData = os.path.join(pathPerson, Folders[idx_file])\n",
    "            \n",
    "            \n",
    "            print(csvFiles[idx_file].split('.')[0])\n",
    "            \n",
    "            # 데이터 읽기\n",
    "            data_l, data_r, time, initialContactTime, safe_start, safe_end, pevis2Lankle, pevis2Rankle = ReadCSV(pathCSV)\n",
    "            l_contact, l_off, r_contact, r_off, parameters = ReadForceData(pathForceData)\n",
    "            \n",
    "            \n",
    "            # 싱크 맞추기\n",
    "            firstContact = min(r_contact[0], l_contact[0]) # zebris 기준 최초 contact 찾음\n",
    "            timeGap = initialContactTime - firstContact # sync.kinect 기준으로 모든 시간 변환\n",
    "            l_contact = l_contact + timeGap\n",
    "            l_off = l_off + timeGap\n",
    "            r_contact = r_contact + timeGap\n",
    "            r_off = r_off + timeGap\n",
    "\n",
    "            # 정답 라벨\n",
    "            l_contact_ = []\n",
    "            for i in l_contact:\n",
    "                tmp = abs(time - i)\n",
    "                l_contact_.append(np.argmin(tmp))\n",
    "            l_off_ = []\n",
    "            for i in l_off:\n",
    "                tmp = abs(time - i)\n",
    "                l_off_.append(np.argmin(tmp))\n",
    "\n",
    "            l_label = np.zeros(data_l.shape[0])\n",
    "            for i in range(len(l_contact_)):\n",
    "                for j in range(l_contact_[i], l_off_[i]):\n",
    "                    l_label[j] = 1\n",
    "\n",
    "            r_contact_ = []\n",
    "            for i in r_contact:\n",
    "                tmp = abs(time - i)\n",
    "                r_contact_.append(np.argmin(tmp))\n",
    "            r_off_ = []\n",
    "            for i in r_off:\n",
    "                tmp = abs(time - i)\n",
    "                r_off_.append(np.argmin(tmp))\n",
    "\n",
    "            r_label = np.zeros(data_r.shape[0])\n",
    "            for i in range(len(r_contact_)):\n",
    "                for j in range(r_contact_[i], r_off_[i]):\n",
    "                    r_label[j] = 1\n",
    "            \n",
    "            finalAnswer_l, finalAnswer_r, modelOutput_l, modelOutput_r = None, None, None, None\n",
    "            ## 인공지능 결과\n",
    "            if modelBased:\n",
    "                finalAnswer_l, finalAnswer_r, modelOutput_l, modelOutput_r = ModelTest(numFrame = numFrame, pathCSV = pathCSV, model = model, PostProcessing_ = True)\n",
    "            else:\n",
    "                finalAnswer_l, finalAnswer_r, modelOutput_l, modelOutput_r = ruleBased(numFrame = numFrame, safe_start=safe_start, safe_end=safe_end, pevis2Lankle=pevis2Lankle, pevis2Rankle=pevis2Rankle, PostProcessing_=False)\n",
    "            \n",
    " \n",
    "\n",
    "            # Left gait events \n",
    "            l_contact_predict = []\n",
    "            l_off_predict = []\n",
    "            \n",
    "            prev = finalAnswer_l[0]\n",
    "            for i, v in enumerate(finalAnswer_l):\n",
    "                if prev != v:\n",
    "                    if prev == 0:\n",
    "                        l_contact_predict.append(i)\n",
    "                    elif prev == 1:\n",
    "                        l_off_predict.append(i)\n",
    "                prev = v\n",
    "                \n",
    "            # Right gait events \n",
    "            r_contact_predict = []\n",
    "            r_off_predict = []\n",
    "            \n",
    "            prev = finalAnswer_r[0]\n",
    "            for i, v in enumerate(finalAnswer_r):\n",
    "                if prev != v:\n",
    "                    if prev == 0:\n",
    "                        r_contact_predict.append(i)\n",
    "                    elif prev == 1:\n",
    "                        r_off_predict.append(i)\n",
    "                prev = v\n",
    "\n",
    "            # Drawing\n",
    "            if drawing:\n",
    "                x_new = np.linspace(time[0], time[-1], len(time))\n",
    "                colorList = ['b', 'g', 'r', 'c', 'm', 'y']\n",
    "                if modelBased:\n",
    "                    ## Type 1 : Deep learning Input + Label + Prediction\n",
    "                    # Left Input\n",
    "                    fig, axs = plt.subplots(3, figsize=(18,12))\n",
    "                    axs[0].set_title('Left')\n",
    "                    for i in range(data_l.shape[-1]):\n",
    "                        axs[0].plot(x_new, data_l[:,i], c = colorList[i])\n",
    "                    for i in l_contact_:\n",
    "                        axs[0].axvline(x = time[i], c='r')\n",
    "                    for i in l_off_:\n",
    "                        axs[0].axvline(x = time[i], c='b')\n",
    "                    axs[0].legend()\n",
    "\n",
    "                    axs[1].set_title('True Label')\n",
    "                    axs[1].axvline(x = time[safe_start], label='safe Start = {}'.format(safe_start), c='g')\n",
    "                    axs[1].axvline(x = time[safe_end], label='safe End = {}'.format(safe_end), c='g')\n",
    "                    for i in l_contact_:\n",
    "                        axs[1].axvline(x = time[i], label='LContact = {}'.format(i), c='r')\n",
    "                    for i in l_off_:\n",
    "                        axs[1].axvline(x = time[i], label='LOff = {}'.format(i), c='b')\n",
    "\n",
    "                    for i, v in enumerate(l_label):\n",
    "                        if v == 1:\n",
    "                            if i == 0:\n",
    "                                axs[1].plot(time[i], 1, 'b.', label='Swing')\n",
    "                                axs[1].plot(time[i], 1, 'r.', label='Stance')\n",
    "                            else:\n",
    "                                axs[1].plot(time[i], 1, 'r.')\n",
    "                        else:\n",
    "                            if i == 0:\n",
    "                                axs[1].plot(time[i], 0, 'r.', label='Stance')\n",
    "                                axs[1].plot(time[i], 0, 'b.', label='Swing')\n",
    "                            else:\n",
    "                                axs[1].plot(time[i], 0, 'b.')\n",
    "                    axs[1].legend()\n",
    "\n",
    "                    ## Left PostProcessing\n",
    "                    axs[2].set_title('Prediction')\n",
    "                    axs[2].axvline(x = time[safe_start], c='g')\n",
    "                    axs[2].axvline(x = time[safe_end], c='g')\n",
    "                    for i, v in enumerate(finalAnswer_l):\n",
    "                        if v == 1:\n",
    "                            axs[2].plot(time[i + safe_start + numFrame - 1], 1, 'r.')\n",
    "                        else:\n",
    "                            axs[2].plot(time[i + safe_start + numFrame - 1], 0, 'b.')\n",
    "                    for i in l_contact_predict:\n",
    "                        axs[2].axvline(x = time[i + safe_start + numFrame - 1], label='LContact = {}'.format(i), c='r')\n",
    "                    for i in l_off_predict:\n",
    "                        axs[2].axvline(x = time[i + safe_start + numFrame - 1], label='LOff = {}'.format(i), c='b')\n",
    "                    axs[2].legend()\n",
    "\n",
    "                    for ax in axs:\n",
    "                        ax.set_xlim([time[0], time[-1]])  \n",
    "\n",
    "                    fig.tight_layout()\n",
    "                    plt.savefig(saveRoot + '//' + csvFiles[idx_file].split('.')[0] + '_Left_Modelbased.jpg')\n",
    "                    plt.close()\n",
    "\n",
    "\n",
    "                    # Right Input\n",
    "                    fig, axs = plt.subplots(3, figsize=(18,12))\n",
    "                    axs[0].set_title('Right')\n",
    "                    for i in range(data_r.shape[-1]):\n",
    "                        axs[0].plot(x_new, data_r[:,i], c = colorList[i])\n",
    "                    for i in r_contact_:\n",
    "                        axs[0].axvline(x = time[i], c='r')\n",
    "                    for i in r_off_:\n",
    "                        axs[0].axvline(x = time[i], c='b')\n",
    "                    axs[0].legend()\n",
    "\n",
    "                    axs[1].set_title('True Label')\n",
    "                    axs[1].axvline(x = time[safe_start], label='safe Start = {}'.format(safe_start), c='g')\n",
    "                    axs[1].axvline(x = time[safe_end], label='safe End = {}'.format(safe_end), c='g')\n",
    "                    for i in r_contact_:\n",
    "                        axs[1].axvline(x = time[i], label='RContact = {}'.format(i), c='r')\n",
    "                    for i in r_off_:\n",
    "                        axs[1].axvline(x = time[i], label='ROff = {}'.format(i), c='b')\n",
    "\n",
    "                    for i, v in enumerate(r_label):\n",
    "                        if v == 1:\n",
    "                            if i == 0:\n",
    "                                axs[1].plot(time[i], 1, 'b.', label='Swing')\n",
    "                                axs[1].plot(time[i], 1, 'r.', label='Stance')\n",
    "                            else:\n",
    "                                axs[1].plot(time[i], 1, 'r.')\n",
    "                        else:\n",
    "                            if i == 0:\n",
    "                                axs[1].plot(time[i], 0, 'r.', label='Stance')\n",
    "                                axs[1].plot(time[i], 0, 'b.', label='Swing')\n",
    "                            else:\n",
    "                                axs[1].plot(time[i], 0, 'b.')\n",
    "                    axs[1].legend()\n",
    "\n",
    "                    ## Right PostProcessing\n",
    "                    axs[2].set_title('Prediction')\n",
    "                    axs[2].axvline(x = time[safe_start], c='g')\n",
    "                    axs[2].axvline(x = time[safe_end], c='g')\n",
    "                    for i, v in enumerate(finalAnswer_r):\n",
    "                        if v == 1:\n",
    "                            axs[2].plot(time[i + safe_start + numFrame - 1], 1, 'r.')\n",
    "                        else:\n",
    "                            axs[2].plot(time[i + safe_start + numFrame - 1], 0, 'b.')\n",
    "                    for i in r_contact_predict:\n",
    "                        axs[2].axvline(x = time[i + safe_start + numFrame - 1], label='RContact = {}'.format(i), c='r')\n",
    "                    for i in r_off_predict:\n",
    "                        axs[2].axvline(x = time[i + safe_start + numFrame - 1], label='ROff = {}'.format(i), c='b')\n",
    "                    axs[2].legend()\n",
    "\n",
    "                    for ax in axs:\n",
    "                        ax.set_xlim([time[0], time[-1]])  \n",
    "\n",
    "                    fig.tight_layout()\n",
    "                    plt.savefig(saveRoot + '//' + csvFiles[idx_file].split('.')[0] + '_Right_Modelbased.jpg')\n",
    "                    plt.close()\n",
    "                    \n",
    "                else:\n",
    "                    ## Type 2 : Rule Based Input + Label + Prediction\n",
    "                    # Left Input\n",
    "                    fig, axs = plt.subplots(3, figsize=(18,12))\n",
    "                    axs[0].set_title('Left')\n",
    "                    axs[0].plot(x_new, pevis2Lankle[:,-1], 'k-', label = 'pevis2Lankle')\n",
    "                    for i in l_contact_:\n",
    "                        axs[0].axvline(x = time[i], c='r')\n",
    "                    for i in l_off_:\n",
    "                        axs[0].axvline(x = time[i], c='b')\n",
    "                    axs[0].set_ylim([np.min(pevis2Lankle[:,-1]), np.max(pevis2Lankle[:,-1])]) \n",
    "                    \n",
    "                    axs[0].axvline(x = time[np.argmax(pevis2Lankle[:,-1])], c = 'k', ls='--')\n",
    "                    axs[0].axvline(x = time[np.argmin(pevis2Lankle[:,-1])], c = 'k', ls='--')\n",
    "                    \n",
    "                    axs[0].legend()\n",
    "\n",
    "                    axs[1].set_title('True Label')\n",
    "                    axs[1].axvline(x = time[safe_start], label='safe Start = {}'.format(safe_start), c='g')\n",
    "                    axs[1].axvline(x = time[safe_end], label='safe End = {}'.format(safe_end), c='g')\n",
    "                    for i in l_contact_:\n",
    "                        axs[1].axvline(x = time[i], label='LContact = {}'.format(i), c='r')\n",
    "                    for i in l_off_:\n",
    "                        axs[1].axvline(x = time[i], label='LOff = {}'.format(i), c='b')\n",
    "\n",
    "                    for i, v in enumerate(l_label):\n",
    "                        if v == 1:\n",
    "                            if i == 0:\n",
    "                                axs[1].plot(time[i], 1, 'b.', label='Swing')\n",
    "                                axs[1].plot(time[i], 1, 'r.', label='Stance')\n",
    "                            else:\n",
    "                                axs[1].plot(time[i], 1, 'r.')\n",
    "                        else:\n",
    "                            if i == 0:\n",
    "                                axs[1].plot(time[i], 0, 'r.', label='Stance')\n",
    "                                axs[1].plot(time[i], 0, 'b.', label='Swing')\n",
    "                            else:\n",
    "                                axs[1].plot(time[i], 0, 'b.')\n",
    "                    axs[1].legend()\n",
    "\n",
    "                    ## Left PostProcessing\n",
    "                    axs[2].set_title('Prediction')\n",
    "                    axs[2].axvline(x = time[safe_start], c='g')\n",
    "                    axs[2].axvline(x = time[safe_end], c='g')\n",
    "                    for i, v in enumerate(finalAnswer_l):\n",
    "                        if v == 1:\n",
    "                            axs[2].plot(time[i + safe_start + numFrame - 1], 1, 'r.')\n",
    "                        else:\n",
    "                            axs[2].plot(time[i + safe_start + numFrame - 1], 0, 'b.')\n",
    "                    for i in l_contact_predict:\n",
    "                        axs[2].axvline(x = time[i + safe_start + numFrame - 1], label='LContact = {}'.format(i), c='r')\n",
    "                    for i in l_off_predict:\n",
    "                        axs[2].axvline(x = time[i + safe_start + numFrame - 1], label='LOff = {}'.format(i), c='b')\n",
    "                    axs[2].legend()\n",
    "\n",
    "                    for ax in axs:\n",
    "                        ax.set_xlim([time[0], time[-1]])  \n",
    "\n",
    "                    fig.tight_layout()\n",
    "                    plt.savefig(saveRoot + '//' + csvFiles[idx_file].split('.')[0] + '_Left_Rulebased.jpg')\n",
    "                    plt.close()\n",
    "\n",
    "                    # Right Input\n",
    "                    fig, axs = plt.subplots(3, figsize=(18,12))\n",
    "                    axs[0].set_title('Right')\n",
    "                    axs[0].plot(pevis2Rankle[:,-1], 'k-', label = 'pevis2Rankle')\n",
    "                    for i in r_contact_:\n",
    "                        axs[0].axvline(x = time[i], c='r')\n",
    "                    for i in r_off_:\n",
    "                        axs[0].axvline(x = time[i], c='b')\n",
    "                    axs[0].legend()\n",
    "\n",
    "                    axs[1].set_title('True Label')\n",
    "                    axs[1].axvline(x = time[safe_start], label='safe Start = {}'.format(safe_start), c='g')\n",
    "                    axs[1].axvline(x = time[safe_end], label='safe End = {}'.format(safe_end), c='g')\n",
    "                    for i in r_contact_:\n",
    "                        axs[1].axvline(x = time[i], label='LContact = {}'.format(i), c='r')\n",
    "                    for i in r_off_:\n",
    "                        axs[1].axvline(x = time[i], label='LOff = {}'.format(i), c='b')\n",
    "\n",
    "                    for i, v in enumerate(r_label):\n",
    "                        if v == 1:\n",
    "                            if i == 0:\n",
    "                                axs[1].plot(time[i], 1, 'b.', label='Swing')\n",
    "                                axs[1].plot(time[i], 1, 'r.', label='Stance')\n",
    "                            else:\n",
    "                                axs[1].plot(time[i], 1, 'r.')\n",
    "                        else:\n",
    "                            if i == 0:\n",
    "                                axs[1].plot(time[i], 0, 'r.', label='Stance')\n",
    "                                axs[1].plot(time[i], 0, 'b.', label='Swing')\n",
    "                            else:\n",
    "                                axs[1].plot(time[i], 0, 'b.')\n",
    "                    axs[1].legend()\n",
    "\n",
    "                    ## Right PostProcessing\n",
    "                    axs[2].set_title('Prediction')\n",
    "                    axs[2].axvline(x = time[safe_start], c='g')\n",
    "                    axs[2].axvline(x = time[safe_end], c='g')\n",
    "                    for i, v in enumerate(finalAnswer_r):\n",
    "                        if v == 1:\n",
    "                            axs[2].plot(time[i + safe_start + numFrame - 1], 1, 'r.')\n",
    "                        else:\n",
    "                            axs[2].plot(time[i + safe_start + numFrame - 1], 0, 'b.')\n",
    "                    for i in r_contact_predict:\n",
    "                        axs[2].axvline(x = time[i + safe_start + numFrame - 1], label='RContact = {}'.format(i), c='r')\n",
    "                    for i in r_off_predict:\n",
    "                        axs[2].axvline(x = time[i + safe_start + numFrame - 1], label='ROff = {}'.format(i), c='b')\n",
    "                    axs[2].legend()\n",
    "\n",
    "                    fig.tight_layout()\n",
    "\n",
    "                    for ax in axs:\n",
    "                        ax.set_xlim([time[0], time[-1]])  \n",
    "\n",
    "\n",
    "                    plt.savefig(saveRoot + '//' + csvFiles[idx_file].split('.')[0] + '_Right_Rulebased.jpg')\n",
    "                    plt.close()\n",
    "                    \n",
    "            # Gait Event True\n",
    "            # Left\n",
    "            l_label = l_label[safe_start + numFrame - 1:safe_end + 1]\n",
    "            l_contact_true = []\n",
    "            l_off_true = []\n",
    "            \n",
    "            prev = l_label[0]\n",
    "            for i, v in enumerate(l_label):\n",
    "                if prev != v:\n",
    "                    if prev == 0:\n",
    "                        l_contact_true.append(i-1)\n",
    "                    elif prev == 1:\n",
    "                        l_off_true.append(i-1)\n",
    "                prev = v\n",
    "            # Right\n",
    "            r_label = r_label[safe_start + numFrame - 1:safe_end + 1]\n",
    "            r_contact_true = []\n",
    "            r_off_true = []\n",
    "            \n",
    "            prev = r_label[0]\n",
    "            for i, v in enumerate(r_label):\n",
    "                if prev != v:\n",
    "                    if prev == 0:\n",
    "                        r_contact_true.append(i-1)\n",
    "                    elif prev == 1:\n",
    "                        r_off_true.append(i-1)\n",
    "                prev = v\n",
    "                \n",
    "            # Coverage\n",
    "            # Left Contact\n",
    "            if len(l_contact_predict) == len(l_contact_true):\n",
    "                coverage_positive_contact += 1\n",
    "            else:\n",
    "                coverage_negative_contact_list.append(csvFiles[idx_file].split('.')[0])\n",
    "\n",
    "            # Left Toe Off\n",
    "            if len(l_off_predict) == len(l_off_true):\n",
    "                coverage_positive_off += 1\n",
    "            else:\n",
    "                coverage_negative_off_list.append(csvFiles[idx_file].split('.')[0])\n",
    "            \n",
    "            # Right Contact\n",
    "            if len(r_contact_predict) == len(r_contact_true):\n",
    "                coverage_positive_contact += 1\n",
    "            else:\n",
    "                coverage_negative_contact_list.append(csvFiles[idx_file].split('.')[0])\n",
    "            \n",
    "            # Right Toe Off\n",
    "            if len(r_off_predict) == len(r_off_true):\n",
    "                coverage_positive_off += 1\n",
    "            else:\n",
    "                coverage_negative_off_list.append(csvFiles[idx_file].split('.')[0])\n",
    "                \n",
    "            # Frame Error for Contact\n",
    "            if len(l_contact_predict) > 0 and len(l_contact_true) > 0:\n",
    "                for i in range(len(l_contact_predict)):\n",
    "                    tmpList_abs = [abs(x - l_contact_predict[i]) for x in l_contact_true]    \n",
    "                    minIndex = np.argmin(tmpList_abs)\n",
    "                    FrameError_contact.append(l_contact_predict[i] - l_contact_true[minIndex])\n",
    "                    \n",
    "            if len(r_contact_predict) > 0 and len(r_contact_true) > 0:\n",
    "                for i in range(len(r_contact_predict)):\n",
    "                    tmpList_abs = [abs(x - r_contact_predict[i]) for x in r_contact_true]  \n",
    "                    minIndex = np.argmin(tmpList_abs)\n",
    "                    FrameError_contact.append(r_contact_predict[i] - r_contact_true[minIndex])\n",
    "\n",
    "            # Frame Error for Toe off\n",
    "            if len(l_off_predict) > 0 and len(l_off_true) > 0: \n",
    "                for i in range(len(l_off_predict)):\n",
    "                    tmpList_abs = [abs(x - l_off_predict[i]) for x in l_off_true]  \n",
    "                    minIndex = np.argmin(tmpList_abs)\n",
    "                    FrameError_off.append(l_off_predict[i] - l_off_true[minIndex])\n",
    "                    \n",
    "            if len(r_off_predict) > 0 and len(r_off_true) > 0: \n",
    "                for i in range(len(r_off_predict)):\n",
    "                    tmpList_abs = [abs(x - r_off_predict[i]) for x in r_off_true]  \n",
    "                    minIndex = np.argmin(tmpList_abs)\n",
    "                    FrameError_off.append(r_off_predict[i] - r_off_true[minIndex])\n",
    "                \n",
    "            # classification Acc\n",
    "            totalFrame += len(l_label)\n",
    "            for i in range(len(l_label)):\n",
    "                if l_label[i] == finalAnswer_l[i]:\n",
    "                    classification_positive += 1\n",
    "            totalFrame += len(r_label)\n",
    "            for i in range(len(r_label)):\n",
    "                if r_label[i] == finalAnswer_r[i]:\n",
    "                    classification_positive += 1\n",
    "                    \n",
    "            # Gait Parameters, Zebris에서 제공하는 값. 일단은 안씀\n",
    "            stepLength_L = parameters.values.tolist()[0][0]\n",
    "            stancePhase_L = parameters.values.tolist()[0][1]\n",
    "            \n",
    "            stepLength_R = parameters.values.tolist()[0][2]\n",
    "            stancePhase_R = parameters.values.tolist()[0][3]\n",
    "            \n",
    "            strideLength = parameters.values.tolist()[0][4]\n",
    "            Cadence = parameters.values.tolist()[0][5]\n",
    "            Velocity = parameters.values.tolist()[0][6]\n",
    "\n",
    "            LstrideLength_true, RstrideLength_true, LstepLength_true, RstepLength_true = PredictParamters(pathCSV, l_contact_true, l_off_true, r_contact_true, r_off_true, safe_start)\n",
    "            LstrideLength_pred, RstrideLength_pred, LstepLength_pred, RstepLength_pred = PredictParamters(pathCSV, l_contact_predict, l_off_predict, r_contact_predict, r_off_predict, safe_start)\n",
    "                           \n",
    "            if LstrideLength_true is not None and LstrideLength_pred is not None:\n",
    "                Error_strideLength_L.append(LstrideLength_true - LstrideLength_pred)\n",
    "                count_strideLength_L += 1\n",
    "            if RstrideLength_true is not None and RstrideLength_pred is not None:\n",
    "                Error_strideLength_R.append(RstrideLength_true - RstrideLength_pred)\n",
    "                count_strideLength_R += 1\n",
    "                \n",
    "            if LstepLength_true is not None and LstepLength_pred is not None:\n",
    "                Error_stepLength_L.append(LstepLength_true - LstepLength_pred)\n",
    "                count_stepLength_L += 1\n",
    "            if RstepLength_true is not None and RstepLength_pred is not None:\n",
    "                Error_stepLength_R.append(RstepLength_true - RstepLength_pred)\n",
    "                count_stepLength_R += 1\n",
    "                \n",
    "            numTrial += 2\n",
    "            \n",
    "    print('contact negative')\n",
    "    print(coverage_negative_contact_list)\n",
    "    print('off negative')\n",
    "    print(coverage_negative_off_list)\n",
    "    classificationAcc_final = classification_positive * 100 / totalFrame\n",
    "    coverageAcc_contact_final = coverage_positive_contact * 100 / numTrial\n",
    "    coverageAcc_off_final = coverage_positive_off * 100 / numTrial\n",
    "    \n",
    "    \n",
    "    FrameError_contact_mean_final = np.mean(FrameError_contact)\n",
    "    FrameError_contact_std_final = np.std(FrameError_contact)\n",
    "    \n",
    "    FrameError_off_mean_final = np.mean(FrameError_off)\n",
    "    FrameError_off_std_final = np.std(FrameError_off)\n",
    "        \n",
    "    finalError_stepLength_L_mean = 0\n",
    "    finalError_stepLength_L_std = 0\n",
    "    if count_stepLength_L > 0:\n",
    "        finalError_stepLength_L_mean = np.mean(Error_stepLength_L)\n",
    "        finalError_stepLength_L_std = np.std(Error_stepLength_L)\n",
    "        \n",
    "    finalError_stepLength_R_mean = 0\n",
    "    finalError_stepLength_R_std = 0\n",
    "    if count_stepLength_R > 0:\n",
    "        finalError_stepLength_R_mean = np.mean(Error_stepLength_R)\n",
    "        finalError_stepLength_R_std = np.std(Error_stepLength_R)\n",
    "        \n",
    "    Error_stepLength_L.extend(Error_stepLength_R)\n",
    "    finalError_stepLength_total_mean = np.mean(Error_stepLength_L)\n",
    "    finalError_stepLength_total_std = np.std(Error_stepLength_L)\n",
    "        \n",
    "    finalError_strideLength_L_mean = 0\n",
    "    finalError_strideLength_L_std = 0\n",
    "    if count_strideLength_L > 0:\n",
    "        finalError_strideLength_L_mean = np.mean(Error_strideLength_L)\n",
    "        finalError_strideLength_L_std = np.std(Error_strideLength_L)\n",
    "        \n",
    "    finalError_strideLength_R_mean = 0\n",
    "    finalError_strideLength_R_std = 0\n",
    "    if count_strideLength_R > 0:\n",
    "        finalError_strideLength_R_mean = np.mean(Error_strideLength_R)\n",
    "        finalError_strideLength_R_std = np.std(Error_strideLength_R)\n",
    "        \n",
    "    f = open(saveRoot + '//' + \"_Result.txt\", 'w')\n",
    "    tmp = f'classificationAcc_final : {classificationAcc_final:.3f} \\ncoverageAcc_contact_final : {coverageAcc_contact_final:.3f} \\\n",
    "    \\ncoverageAcc_off_final : {coverageAcc_off_final:.3f} \\nFrameError_contact_mean_final : {FrameError_contact_mean_final:.3f} \\\n",
    "    \\nFrameError_contact_std_final : {FrameError_contact_std_final:.3f} \\nFrameError_off_mean_final : {FrameError_off_mean_final:.3f} \\\n",
    "    \\nFrameError_off_std_final : {FrameError_off_std_final:.3f} \\nfinalError_stepLength_total_mean : {finalError_stepLength_total_mean:.3f} \\\n",
    "    \\nfinalError_stepLength_total_std : {finalError_stepLength_total_std:.3f} \\nfinalError_stepLength_L_mean(cm) : {finalError_stepLength_L_mean:.3f} \\\n",
    "    \\nfinalError_stepLength_L_std(cm) : {finalError_stepLength_L_std:.3f} \\nfinalError_stepLength_R_mean(cm) : {finalError_stepLength_R_mean:.3f} \\\n",
    "    \\nfinalError_stepLength_R_std(cm) : {finalError_stepLength_R_std:.3f} \\nfinalError_strideLength_L_mean(cm) : {finalError_strideLength_L_mean:.3f} \\\n",
    "    \\nfinalError_strideLength_L_std(cm) : {finalError_strideLength_L_std:.3f} \\nfinalError_strideLength_R_mean(cm) : {finalError_strideLength_R_mean:.3f} \\\n",
    "    \\nfinalError_strideLength_R_std(cm) : {finalError_strideLength_R_std:.3f} \\nInputType : {inputType} \\nstopped_epoch : {stopped_epoch}'\n",
    "    f.write(str(tmp))\n",
    "    f.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd9c47fb-6794-41ec-b1f9-028705f6d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train(True)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        model.zero_grad()\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        h0 = torch.zeros(1, X.size(0), hidden_size).to(device)\n",
    "        c0 = torch.zeros(1, X.size(0), hidden_size).to(device)\n",
    "        pred = model(X, h0.detach(), c0.detach())\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \"\"\"\n",
    "        if batch % 400 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \"\"\"\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            h0 = torch.zeros(1, X.size(0), hidden_size).to(device)\n",
    "            c0 = torch.zeros(1, X.size(0), hidden_size).to(device)\n",
    "            pred = model(X, h0.detach(), c0.detach())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    #correct /= size\n",
    "    #print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "731506d4-a9b2-4ee9-9a58-d521abfc8520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPATH = \\'Mymodel_type5_old\\'\\nmodel.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\\nmodel.to(device)\\n\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습에 사용할 CPU나 GPU 장치를 얻습니다.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#print(\"Using {} device\".format(device))\n",
    "#device = \"cpu\"\n",
    "\n",
    "class tcnPhase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(tcnPhase, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=inputDims, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(3, 3, 3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(3, 3, 3, stride=1, padding=1)\n",
    "        \n",
    "        self.input_size = 3\n",
    "        self.hidden_size = 3\n",
    "        self.num_layers = 1 \n",
    "        self.bidirectional = False\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            self.fc = nn.Linear(self.hidden_size * 2, 1)\n",
    "        else:\n",
    "            self.fc = nn.Linear(self.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x, h0, c0):\n",
    "        #x = torch.swapaxes(x, 1, 2)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.conv3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        #x = torch.swapaxes(x, 1, 2)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        #c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        #print(x.shape)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = out[:,-1,:]\n",
    "        \n",
    "        x = self.fc(out)\n",
    "        x = torch.sigmoid(x)\n",
    "        #return out, (hn, cn)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "input_test = torch.randn(7, 10, 5)\n",
    "h0 = torch.zeros(2, 7, 3)\n",
    "c0 = torch.zeros(2, 7, 3)\n",
    "output = model(input_test, h0, c0)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "PATH = 'Mymodel_type5_old'\n",
    "model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c69e7d-cf52-4d6d-b956-ed75375fd6bb",
   "metadata": {},
   "source": [
    "## InputType \n",
    "#0 : (angle_RHip, angle_Rknee, angle_LHip, angle_Lknee, AngkleDistance) \\\n",
    "#1 : (angle_RHip, angle_Rknee, AngkleDistance) \\\n",
    "#2 : (angle_Rknee, angle_Lknee, AngkleDistance) \\\n",
    "#3 : (RHip, RKnee, LHip, LKnee, AngkleDistance) \\\n",
    "#4 : (angle_RHip, angle_Rknee, angle_LHip, angle_Lknee) \\\n",
    "#5 : (angle_RHip, angle_Rknee, angle_LHip, angle_Lknee, AngkleDistance, Head_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f5c7131-8f8c-489d-8205-db82ef2ef081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LJO_00\n",
      "LJO_01\n",
      "LJO_02\n",
      "LJO_03\n",
      "LJO_04\n",
      "LJO_05\n",
      "LJO_06\n",
      "LJO_07\n",
      "LJO_08\n",
      "LJO_09\n",
      "LJO_10\n",
      "LJO_11\n",
      "LJO_12\n",
      "LJO_13\n",
      "LJO_14\n",
      "LJO_15\n",
      "LJO_16\n",
      "LJO_17\n",
      "LJO_18\n",
      "LJO_19\n",
      "LJO_20\n",
      "LJO_21\n",
      "LJO_22\n",
      "LJO_23\n",
      "LJO_24\n",
      "contact negative\n",
      "['LJH_00', 'LJH_01', 'LJH_02', 'LJH_03', 'LJH_05', 'LJH_06', 'LJH_08', 'LJH_09', 'LJH_10', 'LJH_11', 'LJH_13', 'LJH_13', 'LJH_14', 'LJH_16', 'LJH_18', 'LJH_21', 'KGJ_00', 'KGJ_02', 'KGJ_03', 'KGJ_04', 'KGJ_06', 'KGJ_08', 'KGJ_09', 'KGJ_10', 'KGJ_15', 'KGJ_16', 'KGJ_17', 'KGJ_20', 'KGJ_21', 'KGJ_21', 'KGJ_22', 'KGJ_23', 'KGJ_23', 'KGJ_24', 'KGJ_24', 'KGJ_25', 'KGJ_25', 'CIY_00', 'CIY_00', 'CIY_01', 'CIY_02', 'CIY_02', 'CIY_03', 'CIY_04', 'CIY_05', 'CIY_06', 'CIY_06', 'CIY_07', 'CIY_08', 'CIY_09', 'CIY_10', 'CIY_10', 'CIY_11', 'CIY_17', 'CIY_19', 'CIY_20', 'CIY_22', 'PKS_03', 'PKS_05', 'PKS_06', 'PKS_07', 'PKS_08', 'PKS_09', 'PKS_10', 'PKS_11', 'PKS_15', 'PKS_16', 'PKS_19', 'PKS_19', 'PKS_21', 'PKS_22', 'PKS_23', 'KYW_00', 'KYW_02', 'KYW_11', 'KYW_12', 'KYW_14', 'KYW_14', 'KYW_17', 'KYW_18', 'KYW_23', 'PBD_02', 'PBD_02', 'PBD_04', 'PBD_04', 'PBD_05', 'PBD_05', 'PBD_06', 'PBD_06', 'PBD_07', 'PBD_07', 'PBD_08', 'PBD_08', 'PBD_09', 'PBD_09', 'PBD_10', 'PBD_10', 'PBD_11', 'PBD_11', 'PBD_12', 'PBD_12', 'PBD_13', 'PBD_14', 'PBD_15', 'PBD_16', 'PBD_18', 'PBD_19', 'PBD_20', 'PBD_20', 'PBD_21', 'PBD_21', 'PBD_22', 'PBD_23', 'PBD_23', 'PBD_24', 'PBD_24', 'PBD_25', 'PBD_25', 'PBD_27', 'PBD_28', 'YGP_00', 'YGP_00', 'YGP_01', 'YGP_02', 'YGP_04', 'YGP_04', 'YGP_05', 'YGP_05', 'YGP_06', 'YGP_06', 'YGP_07', 'YGP_09', 'YGP_09', 'YGP_10', 'YGP_12', 'YGP_12', 'YGP_13', 'YGP_14', 'YGP_15', 'YGP_16', 'YGP_16', 'YGP_17', 'YGP_18', 'YGP_18', 'YGP_20', 'YGP_20', 'YGP_21', 'YGP_21', 'YGP_22', 'YGP_22', 'YGP_23', 'YGP_23', 'YGD_05', 'YGD_08', 'YGD_10', 'YGD_10', 'YGD_11', 'YGD_17', 'YGD_19', 'YGD_21', 'YGD_23', 'YGD_23', 'YGD_24', 'YGD_24', 'YGD_25', 'YGD_25', 'BGS_00', 'BGS_01', 'BGS_05', 'BGS_09', 'BGS_11', 'BGS_14', 'BGS_14', 'BGS_16', 'BGS_17', 'BGS_21', 'BGS_22', 'LBN_01', 'LBN_02', 'LBN_02', 'LBN_03', 'LBN_03', 'LBN_04', 'LBN_05', 'LBN_05', 'LBN_06', 'LBN_07', 'LBN_07', 'LBN_08', 'LBN_08', 'LBN_09', 'LBN_09', 'LBN_10', 'LBN_10', 'LBN_11', 'LBN_11', 'LBN_12', 'LBN_13', 'LBN_13', 'LBN_14', 'LBN_15', 'LBN_16', 'LBN_16', 'LBN_17', 'LBN_17', 'LBN_18', 'LBN_18', 'LBN_19', 'LBN_20', 'LBN_21', 'LBN_22', 'LBN_22', 'LBN_23', 'LBN_23', 'LBN_24', 'LBN_25', 'LBN_25', 'JSM_00', 'JSM_00', 'JSM_01', 'JSM_02', 'JSM_06', 'JSM_06', 'JSM_07', 'JSM_10', 'JSM_11', 'JSM_12', 'JSM_14', 'JSM_15', 'JSM_17', 'JSM_18', 'JSM_18', 'JSM_19', 'JSM_20', 'JSM_22', 'JSM_23', 'JSM_24', 'JSM_25', 'JSM_25', 'LSH_01', 'LSH_02', 'LSH_03', 'LSH_04', 'LSH_05', 'LSH_06', 'LSH_07', 'LSH_08', 'LSH_09', 'LSH_10', 'LSH_11', 'LSH_13', 'LSH_14', 'LSH_15', 'LSH_16', 'LSH_17', 'LSH_18', 'LSH_19', 'LSH_20', 'LSH_21', 'LSH_22', 'LSH_23', 'LSH_24', 'LSH_24', 'LSH_25', 'LSH_25', 'LSH_26', 'LSH_26', 'SKY_00', 'SKY_03', 'SKY_09', 'SKY_09', 'SKY_16', 'SKY_22', 'SKY_23', 'SKY_24', 'SKY_24', 'RGH_00', 'RGH_03', 'RGH_05', 'RGH_06', 'RGH_08', 'RGH_08', 'RGH_09', 'RGH_16', 'RGH_18', 'RGH_19', 'RGH_20', 'RGH_21', 'RGH_21', 'RGH_23', 'RGH_23', 'RGH_24', 'JBB_00', 'JBB_01', 'JBB_02', 'JBB_03', 'JBB_04', 'JBB_06', 'JBB_07', 'JBB_08', 'JBB_08', 'JBB_09', 'JBB_09', 'JBB_10', 'JBB_11', 'JBB_13', 'JBB_14', 'JBB_15', 'JBB_23', 'JBB_25', 'PSCD_01', 'PSCD_01', 'PSCD_02', 'PSCD_02', 'PSCD_03', 'PSCD_03', 'PSCD_04', 'PSCD_05', 'PSCD_06', 'PSCD_06', 'PSCD_07', 'PSCD_08', 'PSCD_08', 'PSCD_09', 'PSCD_10', 'PSCD_11', 'PSCD_12', 'PSCD_13', 'PSCD_14', 'PSCD_16', 'PSCD_16', 'PSCD_17', 'PSCD_18', 'PSCD_19', 'PSCD_20', 'PSCD_21', 'PSCD_22', 'PSCD_22', 'PSCD_23', 'PSCD_24', 'PSCD_25', 'KCS_00', 'KCS_01', 'KCS_02', 'KCS_03', 'KCS_04', 'KCS_05', 'KCS_05', 'KCS_06', 'KCS_07', 'KCS_11', 'KCS_11', 'KCS_12', 'KCS_13', 'KCS_14', 'KCS_15', 'KCS_17', 'KCS_17', 'KCS_18', 'KCS_19', 'KCS_20', 'KCS_21', 'KCS_22', 'KCS_24', 'KCS_25', 'KSJ_02', 'KSJ_02', 'KSJ_03', 'KSJ_03', 'KSJ_04', 'KSJ_04', 'KSJ_05', 'KSJ_05', 'KSJ_06', 'KSJ_06', 'KSJ_07', 'KSJ_08', 'KSJ_09', 'KSJ_10', 'KSJ_11', 'KSJ_12', 'KSJ_13', 'KSJ_14', 'KSJ_16', 'KSJ_16', 'KSJ_17', 'KSJ_18', 'KSJ_19', 'KSJ_20', 'KSJ_21', 'KSJ_22', 'KSJ_22', 'KSJ_23', 'KSJ_24', 'KSJ_24', 'KSJ_25', 'KSJ_25', 'LJO_01', 'LJO_02', 'LJO_04', 'LJO_04', 'LJO_05', 'LJO_06', 'LJO_08', 'LJO_09', 'LJO_10', 'LJO_11', 'LJO_12', 'LJO_12', 'LJO_14', 'LJO_15', 'LJO_15', 'LJO_16', 'LJO_17', 'LJO_17', 'LJO_18', 'LJO_18', 'LJO_19', 'LJO_19', 'LJO_20', 'LJO_21', 'LJO_21', 'LJO_22', 'LJO_22', 'LJO_23', 'LJO_24', 'LJO_24']\n",
      "off negative\n",
      "['LJH_01', 'LJH_02', 'LJH_08', 'LJH_11', 'LJH_12', 'LJH_13', 'LJH_13', 'LJH_18', 'KGJ_06', 'KGJ_10', 'KGJ_11', 'KGJ_12', 'KGJ_13', 'KGJ_16', 'KGJ_20', 'KGJ_20', 'KGJ_21', 'KGJ_22', 'KGJ_23', 'KGJ_23', 'KGJ_24', 'KGJ_25', 'KGJ_25', 'CIY_00', 'CIY_01', 'CIY_02', 'CIY_03', 'CIY_03', 'CIY_04', 'CIY_05', 'CIY_05', 'CIY_06', 'CIY_07', 'CIY_08', 'CIY_10', 'CIY_12', 'CIY_22', 'PKS_00', 'PKS_07', 'PKS_09', 'PKS_10', 'PKS_23', 'KYW_00', 'KYW_01', 'KYW_02', 'KYW_02', 'KYW_09', 'KYW_10', 'KYW_11', 'KYW_14', 'KYW_18', 'KYW_19', 'KYW_20', 'KYW_21', 'KYW_23', 'KYW_23', 'PBD_00', 'PBD_01', 'PBD_01', 'PBD_02', 'PBD_02', 'PBD_04', 'PBD_04', 'PBD_05', 'PBD_06', 'PBD_07', 'PBD_07', 'PBD_08', 'PBD_09', 'PBD_10', 'PBD_11', 'PBD_12', 'PBD_13', 'PBD_13', 'PBD_15', 'PBD_16', 'PBD_16', 'PBD_18', 'PBD_18', 'PBD_20', 'PBD_20', 'PBD_21', 'PBD_21', 'PBD_22', 'PBD_23', 'PBD_23', 'PBD_24', 'PBD_24', 'PBD_25', 'PBD_25', 'YGP_01', 'YGP_04', 'YGP_05', 'YGP_06', 'YGP_07', 'YGP_11', 'YGP_12', 'YGP_12', 'YGP_15', 'YGP_16', 'YGP_18', 'YGP_19', 'YGP_20', 'YGP_21', 'YGP_22', 'YGP_22', 'YGP_23', 'YGD_10', 'YGD_10', 'YGD_16', 'YGD_23', 'YGD_24', 'YGD_25', 'BGS_05', 'BGS_14', 'LBN_01', 'LBN_02', 'LBN_10', 'LBN_11', 'LBN_13', 'LBN_14', 'LBN_16', 'LBN_17', 'LBN_20', 'LBN_22', 'LBN_22', 'LBN_23', 'LBN_23', 'LBN_25', 'JSM_00', 'JSM_00', 'JSM_03', 'JSM_06', 'JSM_09', 'JSM_25', 'LSH_01', 'LSH_03', 'LSH_07', 'LSH_10', 'LSH_11', 'LSH_13', 'LSH_15', 'LSH_20', 'LSH_24', 'LSH_25', 'LSH_26', 'LSH_26', 'SKY_03', 'SKY_15', 'SKY_22', 'SKY_23', 'SKY_24', 'RGH_00', 'RGH_00', 'RGH_08', 'RGH_16', 'RGH_19', 'RGH_20', 'RGH_21', 'RGH_23', 'RGH_23', 'JBB_01', 'JBB_04', 'JBB_07', 'JBB_08', 'JBB_08', 'JBB_09', 'JBB_26', 'PSCD_01', 'PSCD_01', 'PSCD_02', 'PSCD_02', 'PSCD_03', 'PSCD_06', 'PSCD_08', 'PSCD_16', 'PSCD_16', 'PSCD_18', 'PSCD_21', 'PSCD_22', 'PSCD_22', 'KCS_01', 'KCS_05', 'KCS_06', 'KCS_14', 'KCS_15', 'KCS_17', 'KCS_17', 'KCS_19', 'KCS_20', 'KCS_25', 'KSJ_02', 'KSJ_03', 'KSJ_03', 'KSJ_04', 'KSJ_05', 'KSJ_06', 'KSJ_07', 'KSJ_08', 'KSJ_09', 'KSJ_10', 'KSJ_11', 'KSJ_14', 'KSJ_15', 'KSJ_17', 'KSJ_21', 'LJO_02', 'LJO_03', 'LJO_04', 'LJO_08', 'LJO_10', 'LJO_11', 'LJO_12', 'LJO_15', 'LJO_15', 'LJO_18', 'LJO_23', 'LJO_24']\n"
     ]
    }
   ],
   "source": [
    "inputType = None\n",
    "root = 'Dataset_old'\n",
    "\n",
    "for _ in range(6):\n",
    "    _ = 5 ##\n",
    "    inputType = _\n",
    "    \n",
    "    dataset = CustomDataset(root = root, numFrame = 10, drawing = False)\n",
    "\n",
    "    dataset_size = len(dataset)\n",
    "    train_size = int(0.7 * dataset_size)\n",
    "    test_size = dataset_size - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset,[train_size, test_size])\n",
    "\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "    model = tcnPhase().to(device)\n",
    "    print(model)\n",
    "    \n",
    "    loss_fn = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    hidden_size = 3\n",
    "    proj_size = 1\n",
    "    num_layers = 1\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    EPOCHS = 100000\n",
    "    patience = 30\n",
    "    stopped_epoch = 0\n",
    "    best = np.Inf\n",
    "    wait = 0\n",
    "    \n",
    "    PATH = 'model_' + str(inputType) + '_' + root + '_Result'\n",
    "\n",
    "    lossHistory = []\n",
    "\n",
    "    for t in range(EPOCHS):\n",
    "        clear_output(wait=True)\n",
    "        #if t%50 == 0:\n",
    "        #    clear_output(wait=True)\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        testLoss = test(test_dataloader, model, loss_fn)\n",
    "        lossHistory.append(testLoss)\n",
    "\n",
    "        if testLoss < best:\n",
    "            best = testLoss\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                stopped_epoch = t\n",
    "                device = torch.device(\"cuda\")\n",
    "                model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n",
    "                model.to(device)\n",
    "                break\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "    plt.plot(lossHistory)\n",
    "    plt.title('InputType' + str(inputType))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('InputType' + str(inputType) + '_' + root + '_LossGraph')\n",
    "    plt.close()\n",
    "    \n",
    "    getResults(root=root, numFrame=10, modelBased = True, drawing = False)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72300bab-32d5-43b4-8651-480c17ec29cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tcnPhase().to(device)\n",
    "PATH = 'Mymodel_type5_old'\n",
    "model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n",
    "model.to(device)\n",
    "getResults(root=root, numFrame=10, modelBased = True, drawing = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "382f2095-822c-4cf0-b8f2-5fcbf26cf01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "-------------------------------\n",
      "Avg loss: 0.325458 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7341e39250>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 100000\n",
    "patience = 30\n",
    "stopped_epoch = 0\n",
    "best = np.Inf\n",
    "wait = 0\n",
    "PATH = 'Mymodel'\n",
    "\n",
    "lossHistory = []\n",
    "    \n",
    "for t in range(EPOCHS):\n",
    "    clear_output(wait=True)\n",
    "    #if t%50 == 0:\n",
    "    #    clear_output(wait=True)\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    testLoss = test(test_dataloader, model, loss_fn)\n",
    "    lossHistory.append(testLoss)\n",
    "    \n",
    "    if testLoss < best:\n",
    "        best = testLoss\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            stopped_epoch = t\n",
    "            device = torch.device(\"cuda\")\n",
    "            model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n",
    "            model.to(device)\n",
    "            break\n",
    "    \n",
    "print(\"Done!\")\n",
    "\n",
    "plt.plot(lossHistory)\n",
    "plt.title('InputType' + str(inputType))\n",
    "plt.tight_layout()\n",
    "plt.savefig('InputType' + str(inputType) + '_' + root + '_LossGraph')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7de71709-5465-4b95-8eee-b80171b7eca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GYH_00\n",
      "GYH_01\n",
      "GYH_02\n",
      "GYH_03\n",
      "GYH_04\n",
      "GYH_05\n",
      "GYH_06\n",
      "GYH_07\n",
      "GYH_08\n",
      "GYH_09\n",
      "GYH_10\n",
      "GYH_11\n",
      "GYH_12\n",
      "GYH_13\n",
      "GYH_14\n",
      "GYH_15\n",
      "GYH_16\n",
      "GYH_17\n",
      "GYH_18\n",
      "GYH_19\n",
      "GYH_20\n",
      "GYH_22\n",
      "GYH_23\n",
      "GYH_24\n",
      "GYH_25\n",
      "GYH_26\n",
      "contact negative\n",
      "['LBL_01', 'LBL_03', 'LBL_05', 'LBL_06', 'LBL_07', 'LBL_08', 'LBL_09', 'LBL_10', 'LBL_11', 'LBL_12', 'LBL_13', 'LBL_15', 'LBL_16', 'LBL_18', 'LBL_19', 'LBL_20', 'LBL_21', 'LBL_22', 'LBL_23', 'LBL_24', 'LBL_25', 'LBL_26', 'LBL_28', 'LBL_29', 'PHE_04', 'PHE_06', 'PHE_07', 'PHE_10', 'PHE_11', 'PHE_13', 'PHE_16', 'PHE_19', 'PHE_21', 'PHE_29', 'LKO_21', 'LKO_25', 'SOS_02', 'SOS_03', 'SOS_04', 'SOS_09', 'SOS_12', 'SOS_13', 'SOS_17', 'SOS_23', 'SOS_24', 'SOS_28', 'SOS_29', 'SOS_29', 'KMK_05', 'KMK_05', 'KMK_10', 'KMK_10', 'KMK_14', 'KMK_18', 'KMK_18', 'KMK_19', 'KMK_19', 'LJE_00', 'LJE_13', 'LJE_23', 'HSN_00', 'HSN_03', 'HSN_04', 'HSN_06', 'HSN_07', 'HSN_26', 'KHM_01', 'KHM_08', 'KHM_12', 'KHM_15', 'KHM_16', 'KHM_18', 'KHM_19', 'KHM_20', 'KHM_28', 'LSJ_11', 'LSJ_26', 'LSJ_28', 'BYS_01', 'BYS_05', 'BYS_07', 'BYS_13', 'BYS_15', 'BYS_17', 'BYS_20', 'CSY_22', 'LJS_26', 'GYH_00', 'GYH_01', 'GYH_02', 'GYH_03', 'GYH_03', 'GYH_04', 'GYH_05', 'GYH_06', 'GYH_06', 'GYH_07', 'GYH_08', 'GYH_09', 'GYH_10', 'GYH_10', 'GYH_11', 'GYH_12', 'GYH_13', 'GYH_15', 'GYH_16', 'GYH_17', 'GYH_20', 'GYH_20', 'GYH_23', 'GYH_25', 'GYH_26']\n",
      "off negative\n",
      "['LBL_00', 'LBL_05', 'LBL_10', 'LBL_26', 'PHE_00', 'PHE_24', 'LKO_21', 'LKO_25', 'LKO_27', 'UKO_04', 'UKO_07', 'UKO_07', 'UKO_09', 'UKO_12', 'UKO_13', 'UKO_14', 'SOS_02', 'SOS_03', 'SOS_04', 'SOS_09', 'SOS_13', 'KMK_04', 'KMK_05', 'KMK_05', 'KMK_10', 'KMK_10', 'KMK_14', 'KMK_14', 'KMK_18', 'KMK_18', 'KMK_19', 'KMK_19', 'KMK_24', 'KMK_25', 'KMK_25', 'LJE_00', 'LJE_01', 'LJE_07', 'LJE_08', 'LJE_10', 'LJE_11', 'LJE_12', 'LJE_13', 'LJE_13', 'LJE_14', 'LJE_15', 'LJE_16', 'LJE_18', 'LJE_19', 'LJE_22', 'LJE_23', 'LJE_23', 'LJE_29', 'HSN_01', 'HSN_02', 'HSN_08', 'HSN_09', 'HSN_12', 'HSN_13', 'HSN_14', 'HSN_26', 'HSN_29', 'KHM_00', 'KHM_06', 'KHM_07', 'KHM_08', 'KHM_09', 'KHM_11', 'KHM_12', 'KHM_13', 'KHM_14', 'KHM_15', 'KHM_16', 'KHM_18', 'KHM_19', 'KHM_21', 'KHM_23', 'KHM_26', 'KHM_27', 'KHM_29', 'LSJ_26', 'LSJ_28', 'CSY_22', 'LJS_00', 'LJS_01', 'LJS_07', 'LJS_08', 'LJS_09', 'LJS_11', 'LJS_24', 'PSM_03', 'PSM_28', 'GYH_00', 'GYH_02', 'GYH_13']\n"
     ]
    }
   ],
   "source": [
    "root = 'Dataset'\n",
    "getResults(root=root, numFrame=10, modelBased = False, drawing = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdbef9a-7df7-4447-a09d-4fed7f7d3db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
