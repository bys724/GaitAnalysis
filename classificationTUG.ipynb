{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e5891f-fd1f-492c-a76e-a41fc0a5aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from scipy.signal import butter, filtfilt, find_peaks\n",
    "from torch import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "942a830e-d444-4ebd-906f-558b41d8e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Low Pass Filter for Azure data\n",
    "def butter_lowpass_filter(data, cutoff, order):\n",
    "    normal_cutoff=cutoff/(15) \n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    x = np.expand_dims(filtfilt(b, a, data[:,0]), axis=-1)\n",
    "    y = np.expand_dims(filtfilt(b, a, data[:,1]), axis=-1)\n",
    "    z = np.expand_dims(filtfilt(b, a, data[:,2]), axis=-1)\n",
    "    filtered = np.concatenate((x, y, z), axis=-1)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5254f66-e542-45ab-86bf-f5f0c7373965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadCSV(path):\n",
    "    csv_data = np.loadtxt(path, delimiter=',', skiprows=1, usecols=range(1, 101))\n",
    "    \n",
    "    # LPF를 위한 변수\n",
    "    cutoff = 6\n",
    "    order = 30\n",
    "    \n",
    "    pelvis_idx = 0\n",
    "    \n",
    "    LShoulder_idx = 5\n",
    "    LElbow_idx = 6\n",
    "    LWrist_idx = 7\n",
    "    LHip_idx = 18 \n",
    "    LKnee_idx = 19\n",
    "    LAnkle_idx = 20\n",
    "    \n",
    "    RShoulder_idx = 12\n",
    "    RElbow_idx = 13\n",
    "    RWrist_idx = 14\n",
    "    RHip_idx = 22\n",
    "    RKnee_idx = 23\n",
    "    RAnkle_idx = 24\n",
    "    \n",
    "    Pelvis = csv_data[:,pelvis_idx * 3:(pelvis_idx + 1) * 3]\n",
    "    Pelvis_filtered = butter_lowpass_filter(Pelvis, cutoff, order)\n",
    "    \n",
    "    LShoulder = csv_data[:,LShoulder_idx * 3:(LShoulder_idx + 1) * 3]\n",
    "    LShoulder_filtered = butter_lowpass_filter(LShoulder, cutoff, order)\n",
    "    \n",
    "    LElbow = csv_data[:,LElbow_idx * 3:(LElbow_idx + 1) * 3]\n",
    "    LElbow_filtered = butter_lowpass_filter(LElbow, cutoff, order)\n",
    "    \n",
    "    LWrist = csv_data[:,LWrist_idx * 3:(LWrist_idx + 1) * 3]\n",
    "    LWrist_filtered = butter_lowpass_filter(LWrist, cutoff, order)\n",
    "    \n",
    "    LHip = csv_data[:,LHip_idx * 3:(LHip_idx + 1) * 3]\n",
    "    LHip_filtered = butter_lowpass_filter(LHip, cutoff, order)\n",
    "    \n",
    "    LKnee = csv_data[:,LKnee_idx * 3:(LKnee_idx + 1) * 3]\n",
    "    LKnee_filtered = butter_lowpass_filter(LKnee, cutoff, order)\n",
    "    \n",
    "    LAnkle = csv_data[:,LAnkle_idx * 3:(LAnkle_idx + 1) * 3]\n",
    "    LAnkle_filtered = butter_lowpass_filter(LAnkle, cutoff, order)\n",
    "    \n",
    "    RShoulder = csv_data[:,RShoulder_idx * 3:(RShoulder_idx + 1) * 3]\n",
    "    RShoulder_filtered = butter_lowpass_filter(RShoulder, cutoff, order)\n",
    "    \n",
    "    RElbow = csv_data[:,RElbow_idx * 3:(RElbow_idx + 1) * 3]\n",
    "    RElbow_filtered = butter_lowpass_filter(RElbow, cutoff, order)\n",
    "    \n",
    "    RWrist = csv_data[:,RWrist_idx * 3:(RWrist_idx + 1) * 3]\n",
    "    RWrist_filtered = butter_lowpass_filter(RWrist, cutoff, order)\n",
    "    \n",
    "    RHip = csv_data[:,RHip_idx * 3:(RHip_idx + 1) * 3]\n",
    "    RHip_filtered = butter_lowpass_filter(RHip, cutoff, order)\n",
    "    \n",
    "    RKnee = csv_data[:,RKnee_idx * 3:(RKnee_idx + 1) * 3]\n",
    "    RKnee_filtered = butter_lowpass_filter(RKnee, cutoff, order)\n",
    "    \n",
    "    RAnkle = csv_data[:,RAnkle_idx * 3:(RAnkle_idx + 1) * 3]\n",
    "    RAnkle_filtered = butter_lowpass_filter(RAnkle, cutoff, order)\n",
    "    \n",
    "\n",
    "    Pelvis_vel = Pelvis_filtered[1:,:] - Pelvis_filtered[:-1,:]\n",
    "    LElbow_vel = LElbow_filtered[1:,:] - LElbow_filtered[:-1,:]\n",
    "    LKnee_vel = LKnee_filtered[1:,:] - LKnee_filtered[:-1,:]\n",
    "    RElbow_vel = RElbow_filtered[1:,:] - RElbow_filtered[:-1,:]\n",
    "    RKnee_vel = RKnee_filtered[1:,:] - RKnee_filtered[:-1,:]\n",
    "    \n",
    "    ## Pelvis에 대한 무릎과 팔꿈치의 상대적인 움직임\n",
    "    Pelvis_vel_nomalized = Pelvis_vel / np.linalg.norm(Pelvis_vel, axis=1, keepdims=True)\n",
    "    \n",
    "    LElbow_alongPelvis_norm = np.expand_dims(np.tensordot(Pelvis_vel_nomalized, LElbow_vel, axes=([1],[1])).diagonal(), axis=-1)    \n",
    "    LElbow_alongPelvis = Pelvis_vel_nomalized * LElbow_alongPelvis_norm\n",
    "    LElbow_vertical2Pelvis = LElbow_vel - LElbow_alongPelvis\n",
    "    LElbow_vertical2Pelvis_norm = np.linalg.norm(LElbow_vertical2Pelvis, axis=1, keepdims=True)\n",
    "    \n",
    "    LKnee_alongPelvis_norm = np.expand_dims(np.tensordot(Pelvis_vel_nomalized, LKnee_vel, axes=([1],[1])).diagonal(), axis=-1)    \n",
    "    LKnee_alongPelvis = Pelvis_vel_nomalized * LKnee_alongPelvis_norm\n",
    "    LKnee_vertical2Pelvis = LKnee_vel - LKnee_alongPelvis\n",
    "    LKnee_vertical2Pelvis_norm = np.linalg.norm(LKnee_vertical2Pelvis, axis=1, keepdims=True)\n",
    "    \n",
    "    RElbow_alongPelvis_norm = np.expand_dims(np.tensordot(Pelvis_vel_nomalized, RElbow_vel, axes=([1],[1])).diagonal(), axis=-1)    \n",
    "    RElbow_alongPelvis = Pelvis_vel_nomalized * RElbow_alongPelvis_norm\n",
    "    RElbow_vertical2Pelvis = RElbow_vel - RElbow_alongPelvis\n",
    "    RElbow_vertical2Pelvis_norm = np.linalg.norm(RElbow_vertical2Pelvis, axis=1, keepdims=True)\n",
    "    \n",
    "    RKnee_alongPelvis_norm = np.expand_dims(np.tensordot(Pelvis_vel_nomalized, RKnee_vel, axes=([1],[1])).diagonal(), axis=-1)    \n",
    "    RKnee_alongPelvis = Pelvis_vel_nomalized * RKnee_alongPelvis_norm\n",
    "    RKnee_vertical2Pelvis = RKnee_vel - RKnee_alongPelvis\n",
    "    RKnee_vertical2Pelvis_norm = np.linalg.norm(RKnee_vertical2Pelvis, axis=1, keepdims=True)\n",
    "    \n",
    "    \n",
    "    ## 무릎,팔꿈치 각도, 0번째 데이터는 삭제해야함. 왜냐하면 위의 velocity 가 1번부터 시작\n",
    "    knee2hip = LHip_filtered - LKnee_filtered\n",
    "    knee2hip = knee2hip / np.linalg.norm(knee2hip, axis = 1, keepdims=True)\n",
    "    knee2angkle = LAnkle_filtered - LKnee_filtered\n",
    "    knee2angkle = knee2angkle / np.linalg.norm(knee2angkle, axis = 1, keepdims=True)\n",
    "    inner = np.sum(np.multiply(knee2hip, knee2angkle), axis=-1)\n",
    "    LKneeAngle = np.expand_dims(180 - np.arccos(inner) * 180 / np.pi, axis=-1)[1:,:]\n",
    "    \n",
    "    elbow2wrist = LWrist_filtered - LElbow_filtered\n",
    "    elbow2wrist = elbow2wrist / np.linalg.norm(elbow2wrist, axis = 1, keepdims=True)\n",
    "    elbow2shoulder = LShoulder_filtered - LElbow_filtered\n",
    "    elbow2shoulder = elbow2shoulder / np.linalg.norm(elbow2shoulder, axis = 1, keepdims=True)\n",
    "    inner = np.sum(np.multiply(elbow2wrist, elbow2shoulder), axis=-1)\n",
    "    LElbowAngle = np.expand_dims(180 - np.arccos(inner) * 180 / np.pi, axis=-1)[1:,:]\n",
    "    \n",
    "    knee2hip = RHip_filtered - RKnee_filtered\n",
    "    knee2hip = knee2hip / np.linalg.norm(knee2hip, axis = 1, keepdims=True)\n",
    "    knee2angkle = RAnkle_filtered - RKnee_filtered\n",
    "    knee2angkle = knee2angkle / np.linalg.norm(knee2angkle, axis = 1, keepdims=True)\n",
    "    inner = np.sum(np.multiply(knee2hip, knee2angkle), axis=-1)\n",
    "    RKneeAngle = np.expand_dims(180 - np.arccos(inner) * 180 / np.pi, axis=-1)[1:,:]\n",
    "    \n",
    "    elbow2wrist = RWrist_filtered - RElbow_filtered\n",
    "    elbow2wrist = elbow2wrist / np.linalg.norm(elbow2wrist, axis = 1, keepdims=True)\n",
    "    elbow2shoulder = RShoulder_filtered - RElbow_filtered\n",
    "    elbow2shoulder = elbow2shoulder / np.linalg.norm(elbow2shoulder, axis = 1, keepdims=True)\n",
    "    inner = np.sum(np.multiply(elbow2wrist, elbow2shoulder), axis=-1)\n",
    "    RElbowAngle = np.expand_dims(180 - np.arccos(inner) * 180 / np.pi, axis=-1)[1:,:]\n",
    "    \n",
    "    data = np.concatenate((LKneeAngle, LElbowAngle, RKneeAngle, RElbowAngle, LElbow_alongPelvis_norm, LElbow_vertical2Pelvis_norm, \\\n",
    "                           LKnee_alongPelvis_norm, LKnee_vertical2Pelvis_norm, RElbow_alongPelvis_norm, RElbow_vertical2Pelvis_norm, \\\n",
    "                          RKnee_alongPelvis_norm, RKnee_vertical2Pelvis_norm), axis=-1)  \n",
    "    \n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98826224-e167-494f-9164-51efe74056bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadLabel(path):\n",
    "    dat = open(path) \n",
    "    reader = csv.reader(dat) \n",
    "    line = list(reader)[-1]\n",
    "    answer = []\n",
    "    for i in line:\n",
    "        answer.append(int(i.split('_')[-1].split('.')[0]) - 1)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54726c9a-21f1-4803-93e0-14a96642b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        \n",
    "        global inputDims\n",
    "        \n",
    "        self.root = root\n",
    "        \n",
    "        interval = [15, 0] # 두 수의 합이 인풋의 길이. 앞의 수는 과거부터 현재까지의 프레임 수, 뒤의 수는 미래의 프레임 수 \n",
    "        self.single_inputs = None\n",
    "        self.single_outputs = None\n",
    "        \n",
    "        initBuffer = False\n",
    "        \n",
    "        for name in next(os.walk(self.root))[1]:\n",
    "            #print(name)\n",
    "            for trial in next(os.walk(self.root + '//' + name))[1]:\n",
    "                trialPath = self.root + '//' + name + '//' + trial\n",
    "                \n",
    "                csvPath = trialPath + '//' + 'skeleton_' + name + '_' + trial + '.csv'\n",
    "                labelPath = trialPath + '//' + 'label_' + name + '_' + trial + '.csv'\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    data = ReadCSV(csvPath)\n",
    "                    label = ReadLabel(labelPath)\n",
    "                \n",
    "                    # Label\n",
    "                    # 0 : sit\n",
    "                    # 1 : walking\n",
    "                    # 2 : Turning\n",
    "                    # exceptFrame 액션 사이의 경계를 기준으로 모호한 영역을 지정하고 제외시키기 위한 변수\n",
    "                    exceptFrame = 10\n",
    "\n",
    "                    # startMove까지 sit\n",
    "                    targetFrame = interval[0] - 1\n",
    "                    while (targetFrame + interval[1] + 1) <  (label[0] - exceptFrame):\n",
    "                        single_input = np.expand_dims(data[targetFrame - interval[0] + 1:targetFrame + interval[1] + 1, :], axis = 0)\n",
    "                        single_output = np.expand_dims(np.array([0]), axis=0)\n",
    "\n",
    "                        if not initBuffer:\n",
    "                            self.single_inputs = single_input\n",
    "                            self.single_outputs = single_output\n",
    "                            initBuffer = True\n",
    "                        else:\n",
    "                            self.single_inputs = np.concatenate((self.single_inputs, single_input), axis = 0)\n",
    "                            self.single_outputs = np.concatenate((self.single_outputs, single_output), axis = 0)\n",
    "                        targetFrame += 1\n",
    "\n",
    "                    # startWalk 부터 startTurn 까지 walking\n",
    "                    targetFrame = label[1] + exceptFrame\n",
    "                    while (targetFrame + interval[1] + 1) < (label[2] - exceptFrame):\n",
    "                        single_input = np.expand_dims(data[targetFrame - interval[0] + 1:targetFrame + interval[1] + 1, :], axis = 0)\n",
    "                        single_output = np.expand_dims(np.array([1]), axis=0)\n",
    "\n",
    "                        self.single_inputs = np.concatenate((self.single_inputs, single_input), axis = 0)\n",
    "                        self.single_outputs = np.concatenate((self.single_outputs, single_output), axis = 0)\n",
    "\n",
    "                        targetFrame += 1\n",
    "\n",
    "                    # startTurn 부터 endTurn 까지 turing\n",
    "                    targetFrame = label[2] + exceptFrame\n",
    "                    while (targetFrame + interval[1] + 1) < (label[3] - exceptFrame):\n",
    "                        single_input = np.expand_dims(data[targetFrame - interval[0] + 1:targetFrame + interval[1] + 1, :], axis = 0)\n",
    "                        single_output = np.expand_dims(np.array([2]), axis=0)\n",
    "\n",
    "                        self.single_inputs = np.concatenate((self.single_inputs, single_input), axis = 0)\n",
    "                        self.single_outputs = np.concatenate((self.single_outputs, single_output), axis = 0)\n",
    "\n",
    "                        targetFrame += 1\n",
    "\n",
    "                    # endTurn 부터 startSit 까지 walking\n",
    "                    targetFrame = label[3] + exceptFrame\n",
    "                    while (targetFrame + interval[1] + 1) < (label[4] - exceptFrame):\n",
    "                        single_input = np.expand_dims(data[targetFrame - interval[0] + 1:targetFrame + interval[1] + 1, :], axis = 0)\n",
    "                        single_output = np.expand_dims(np.array([1]), axis=0)\n",
    "\n",
    "                        self.single_inputs = np.concatenate((self.single_inputs, single_input), axis = 0)\n",
    "                        self.single_outputs = np.concatenate((self.single_outputs, single_output), axis = 0)\n",
    "\n",
    "                        targetFrame += 1\n",
    "\n",
    "                    # endSit 이후 sit\n",
    "                    targetFrame = label[5] + exceptFrame\n",
    "                    while (targetFrame + interval[1] + 1) < (data.shape[0]):\n",
    "                        single_input = np.expand_dims(data[targetFrame - interval[0] + 1:targetFrame + interval[1] + 1, :], axis = 0)\n",
    "                        single_output = np.expand_dims(np.array([0]), axis=0)\n",
    "\n",
    "                        self.single_inputs = np.concatenate((self.single_inputs, single_input), axis = 0)\n",
    "                        self.single_outputs = np.concatenate((self.single_outputs, single_output), axis = 0)\n",
    "\n",
    "                        targetFrame += 1\n",
    "                        \n",
    "                except:\n",
    "                    print(csvPath)\n",
    "                    \n",
    "        inputDims = self.single_inputs.shape[-1]\n",
    "        print('for single inputs')           \n",
    "        print(self.single_inputs.shape)\n",
    "        print(self.single_outputs.shape)\n",
    "        \n",
    "        \n",
    "        ## combined\n",
    "        initBuffer = False\n",
    "        self.combined_inputs = None\n",
    "        self.combined_outputs = None\n",
    "        \n",
    "        self.combinedNum = 9000\n",
    "        self.classNum = 3\n",
    "        self.classIdx = []\n",
    "        \n",
    "        for i in range(self.classNum):\n",
    "            idx = np.where(self.single_outputs == i)[0]\n",
    "            self.classIdx.append(idx)\n",
    "        \n",
    "        # for same class combination\n",
    "        for i in range(self.classNum):\n",
    "            numberForclass = int(self.combinedNum / self.classNum)\n",
    "            choosen1_idx = np.random.choice(self.classIdx[i], size=numberForclass, replace=False)\n",
    "            choosen2_idx = np.random.choice(self.classIdx[i], size=numberForclass, replace=False)\n",
    "            \n",
    "            choosen1 = np.expand_dims(self.single_inputs[choosen1_idx], axis=1)\n",
    "            choosen2 = np.expand_dims(self.single_inputs[choosen2_idx], axis=1)\n",
    "            \n",
    "            combined = np.concatenate((choosen1, choosen2), axis = 1)\n",
    "            \n",
    "            if not initBuffer:\n",
    "                self.combined_inputs = combined\n",
    "                self.combined_outputs = np.full((numberForclass, 1), 0)\n",
    "            else:\n",
    "                self.combined_inputs = np.concatenate((self.combined_inputs, combined), axis = 0)\n",
    "                self.combined_outputs = np.concatenate((self.combined_outputs, np.full((numberForclass, 1), 0)), axis = 0)\n",
    "            \n",
    "        \n",
    "        \n",
    "            initBuffer = True\n",
    "        \n",
    "        # for diff class combination\n",
    "        for i in range(self.combinedNum):\n",
    "            choosedClasses = np.random.choice(np.arange(self.classNum), size = 2, replace = False)\n",
    "            choosen1_idx = np.random.choice(self.classIdx[choosedClasses[0]], size=1, replace=False)\n",
    "            choosen2_idx = np.random.choice(self.classIdx[choosedClasses[1]], size=1, replace=False)\n",
    "            \n",
    "            choosen1 = np.expand_dims(self.single_inputs[choosen1_idx], axis=1)\n",
    "            choosen2 = np.expand_dims(self.single_inputs[choosen2_idx], axis=1)\n",
    "            \n",
    "            combined = np.concatenate((choosen1, choosen2), axis = 1)\n",
    "            \n",
    "            self.combined_inputs = np.concatenate((self.combined_inputs, combined), axis = 0)\n",
    "            self.combined_outputs = np.concatenate((self.combined_outputs, np.full((1, 1), 1)), axis = 0)\n",
    "            \n",
    "        # shuffle\n",
    "        s = np.arange(self.combined_inputs.shape[0])\n",
    "        np.random.shuffle(s)\n",
    "        self.combined_inputs = self.combined_inputs[s]\n",
    "        self.combined_outputs = self.combined_outputs[s]\n",
    "        \n",
    "        print('for combined inputs')\n",
    "        print(self.combined_inputs.shape)\n",
    "        print(self.combined_outputs.shape)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.combined_inputs)\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(self.combined_inputs[idx,:,:])\n",
    "        y = torch.FloatTensor([self.combined_outputs[idx]])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "914fa8c5-ee08-42ad-a248-6c19cd3d3e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//01//skeleton_JSM_TUG_01.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//05//skeleton_JSM_TUG_05.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//15//skeleton_JSM_TUG_15.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//08//skeleton_JSM_TUG_08.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//14//skeleton_JSM_TUG_14.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//06//skeleton_JSM_TUG_06.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//10//skeleton_JSM_TUG_10.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//02//skeleton_JSM_TUG_02.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//03//skeleton_JSM_TUG_03.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//09//skeleton_JSM_TUG_09.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//12//skeleton_JSM_TUG_12.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//04//skeleton_JSM_TUG_04.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//07//skeleton_JSM_TUG_07.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//11//skeleton_JSM_TUG_11.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//00//skeleton_JSM_TUG_00.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//JSM_TUG//13//skeleton_JSM_TUG_13.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//01//skeleton_CIY_TUG_01.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//05//skeleton_CIY_TUG_05.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//08//skeleton_CIY_TUG_08.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//14//skeleton_CIY_TUG_14.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//06//skeleton_CIY_TUG_06.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//10//skeleton_CIY_TUG_10.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//02//skeleton_CIY_TUG_02.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//03//skeleton_CIY_TUG_03.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//09//skeleton_CIY_TUG_09.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//12//skeleton_CIY_TUG_12.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//04//skeleton_CIY_TUG_04.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//07//skeleton_CIY_TUG_07.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//11//skeleton_CIY_TUG_11.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//00//skeleton_CIY_TUG_00.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CIY_TUG//13//skeleton_CIY_TUG_13.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LJH_TUG//04//skeleton_LJH_TUG_04.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LJH_TUG//13//skeleton_LJH_TUG_13.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//PKS_TUG//01//skeleton_PKS_TUG_01.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//PKS_TUG//05//skeleton_PKS_TUG_05.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//PKS_TUG//15//skeleton_PKS_TUG_15.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//PKS_TUG//16//skeleton_PKS_TUG_16.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//PKS_TUG//08//skeleton_PKS_TUG_08.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//PKS_TUG//02//skeleton_PKS_TUG_02.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//PKS_TUG//03//skeleton_PKS_TUG_03.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//PKS_TUG//09//skeleton_PKS_TUG_09.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//PKS_TUG//12//skeleton_PKS_TUG_12.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//PKS_TUG//04//skeleton_PKS_TUG_04.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//PKS_TUG//07//skeleton_PKS_TUG_07.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//PKS_TUG//00//skeleton_PKS_TUG_00.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//PKS_TUG//13//skeleton_PKS_TUG_13.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LBN_TUG//00//skeleton_LBN_TUG_00.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CJJ_TUG//01//skeleton_CJJ_TUG_01.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CJJ_TUG//05//skeleton_CJJ_TUG_05.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CJJ_TUG//15//skeleton_CJJ_TUG_15.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CJJ_TUG//14//skeleton_CJJ_TUG_14.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CJJ_TUG//06//skeleton_CJJ_TUG_06.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CJJ_TUG//10//skeleton_CJJ_TUG_10.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CJJ_TUG//02//skeleton_CJJ_TUG_02.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CJJ_TUG//03//skeleton_CJJ_TUG_03.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CJJ_TUG//09//skeleton_CJJ_TUG_09.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CJJ_TUG//04//skeleton_CJJ_TUG_04.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CJJ_TUG//07//skeleton_CJJ_TUG_07.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CJJ_TUG//11//skeleton_CJJ_TUG_11.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CJJ_TUG//00//skeleton_CJJ_TUG_00.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CJJ_TUG//13//skeleton_CJJ_TUG_13.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYW_TUG//15//skeleton_KYW_TUG_15.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYW_TUG//14//skeleton_KYW_TUG_14.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYW_TUG//10//skeleton_KYW_TUG_10.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYW_TUG//04//skeleton_KYW_TUG_04.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYW_TUG//11//skeleton_KYW_TUG_11.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYW_TUG//13//skeleton_KYW_TUG_13.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//RGH_TUG//10//skeleton_RGH_TUG_10.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//RGH_TUG//09//skeleton_RGH_TUG_09.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//RGH_TUG//12//skeleton_RGH_TUG_12.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//RGH_TUG//07//skeleton_RGH_TUG_07.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//RGH_TUG//11//skeleton_RGH_TUG_11.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//RGH_TUG//00//skeleton_RGH_TUG_00.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//RGH_TUG//13//skeleton_RGH_TUG_13.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KGJ_TUG//00//skeleton_KGJ_TUG_00.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LHS_TUG//01//skeleton_LHS_TUG_01.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LHS_TUG//15//skeleton_LHS_TUG_15.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LHS_TUG//14//skeleton_LHS_TUG_14.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LHS_TUG//06//skeleton_LHS_TUG_06.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LHS_TUG//02//skeleton_LHS_TUG_02.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LHS_TUG//03//skeleton_LHS_TUG_03.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LHS_TUG//09//skeleton_LHS_TUG_09.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LHS_TUG//12//skeleton_LHS_TUG_12.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LHS_TUG//04//skeleton_LHS_TUG_04.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LHS_TUG//07//skeleton_LHS_TUG_07.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LHS_TUG//11//skeleton_LHS_TUG_11.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LHS_TUG//00//skeleton_LHS_TUG_00.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LHS_TUG//13//skeleton_LHS_TUG_13.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//BGS_TUG//02//skeleton_BGS_TUG_02.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KJT_TUG//04//skeleton_KJT_TUG_04.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LJO_TUG//00//skeleton_LJO_TUG_00.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KCS_TUG//01//skeleton_KCS_TUG_01.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KCS_TUG//08//skeleton_KCS_TUG_08.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KCS_TUG//06//skeleton_KCS_TUG_06.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KCS_TUG//10//skeleton_KCS_TUG_10.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KCS_TUG//02//skeleton_KCS_TUG_02.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KCS_TUG//03//skeleton_KCS_TUG_03.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KCS_TUG//09//skeleton_KCS_TUG_09.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KCS_TUG//12//skeleton_KCS_TUG_12.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KCS_TUG//04//skeleton_KCS_TUG_04.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KCS_TUG//07//skeleton_KCS_TUG_07.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KCS_TUG//11//skeleton_KCS_TUG_11.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KCS_TUG//00//skeleton_KCS_TUG_00.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KCS_TUG//13//skeleton_KCS_TUG_13.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CBY_TUG//12//skeleton_CBY_TUG_12.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//CBY_TUG//13//skeleton_CBY_TUG_13.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYK_TUG//05//skeleton_KYK_TUG_05.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYK_TUG//14//skeleton_KYK_TUG_14.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYK_TUG//06//skeleton_KYK_TUG_06.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYK_TUG//02//skeleton_KYK_TUG_02.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYK_TUG//03//skeleton_KYK_TUG_03.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYK_TUG//09//skeleton_KYK_TUG_09.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYK_TUG//12//skeleton_KYK_TUG_12.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYK_TUG//04//skeleton_KYK_TUG_04.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYK_TUG//07//skeleton_KYK_TUG_07.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYK_TUG//11//skeleton_KYK_TUG_11.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//KYK_TUG//13//skeleton_KYK_TUG_13.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//YGD_TUG//01//skeleton_YGD_TUG_01.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//YGD_TUG//05//skeleton_YGD_TUG_05.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//YGD_TUG//15//skeleton_YGD_TUG_15.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//YGD_TUG//14//skeleton_YGD_TUG_14.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//YGD_TUG//10//skeleton_YGD_TUG_10.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//YGD_TUG//02//skeleton_YGD_TUG_02.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//YGD_TUG//03//skeleton_YGD_TUG_03.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//YGD_TUG//09//skeleton_YGD_TUG_09.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//YGD_TUG//12//skeleton_YGD_TUG_12.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//YGD_TUG//04//skeleton_YGD_TUG_04.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//YGD_TUG//11//skeleton_YGD_TUG_11.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//YGD_TUG//00//skeleton_YGD_TUG_00.csv\n",
      "TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder//LSH_TUG//04//skeleton_LSH_TUG_04.csv\n",
      "for single inputs\n",
      "(84334, 15, 12)\n",
      "(84334, 1)\n",
      "for combined inputs\n",
      "(18000, 2, 15, 12)\n",
      "(18000, 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset('TUG_dataset//HMM_saveResults_illness_mean//0_sideView//elder')\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "train_dataset, test_dataset = random_split(dataset,[train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5ff947c-856c-43e0-bab8-6f871367b3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# 학습에 사용할 CPU나 GPU 장치를 얻습니다.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "#device = \"cpu\"\n",
    "\n",
    "class tcnPhase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(tcnPhase, self).__init__()\n",
    "        global hidden_size\n",
    "        self.conv1 = nn.Conv1d(in_channels=inputDims, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(3, 3, 3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(3, 3, 3, stride=1, padding=1)\n",
    "        \n",
    "        self.input_size = 3\n",
    "        self.hidden_size = 3\n",
    "        hidden_size = self.hidden_size\n",
    "        self.num_layers = 1 \n",
    "        self.bidirectional = False\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size, hidden_size=self.hidden_size, num_layers=self.num_layers, batch_first=True, bidirectional=self.bidirectional)\n",
    "        \n",
    "        if self.bidirectional:\n",
    "            self.fc = nn.Linear(self.hidden_size * 2, 3)\n",
    "        else:\n",
    "            self.fc = nn.Linear(self.hidden_size, 3)\n",
    "            \n",
    "        self.Boundary=nn.Parameter(torch.randn(1))\n",
    "        self.relu = nn.ReLU()\n",
    "            \n",
    "    def forward(self, x, h0, c0):\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.conv2(x)\n",
    "        #x = torch.sigmoid(x)\n",
    "        x = self.conv3(x)\n",
    "        #x = torch.sigmoid(x)\n",
    "        x = torch.transpose(x, 1, 2)\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        out = out[:,-1,:]\n",
    "        x = self.fc(out)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, p1, p2):\n",
    "        distance = LA.norm(p1 - p2, dim=1, keepdim = True) \n",
    "        Dinominator = distance + self.relu(self.Boundary.repeat(distance.shape)) + torch.full(distance.shape,  1e-10).to(device)\n",
    "        predFinal = distance / Dinominator\n",
    "        return predFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c65551dc-a15d-4219-8b5a-d3e2225692f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train(True)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        input1 = X[:,0,:,:].to(device)\n",
    "        input2 = X[:,1,:,:].to(device)\n",
    "        output = y[:,0,:].to(device)\n",
    "        \n",
    "        h0 = torch.zeros(1, input1.size(0), hidden_size).to(device)\n",
    "        c0 = torch.zeros(1, input1.size(0), hidden_size).to(device)\n",
    "        \n",
    "        pred1 = model(input1, h0.detach(), c0.detach())\n",
    "        pred2 = model(input2, h0.detach(), c0.detach())\n",
    "        \n",
    "        predFinal = model.predict(pred1, pred2)\n",
    "        loss = loss_fn(predFinal, output)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "            \n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            input1 = X[:,0,:,:].to(device)\n",
    "            input2 = X[:,1,:,:].to(device)\n",
    "            output = y[:,0,:].to(device)\n",
    "\n",
    "            h0 = torch.zeros(1, input1.size(0), hidden_size).to(device)\n",
    "            c0 = torch.zeros(1, input1.size(0), hidden_size).to(device)\n",
    "\n",
    "            pred1 = model(input1, h0.detach(), c0.detach()).to(device)\n",
    "            pred2 = model(input2, h0.detach(), c0.detach()).to(device)\n",
    "            \n",
    "            predFinal = model.predict(pred1, pred2)\n",
    "            test_loss += loss_fn(predFinal, output).item()\n",
    "            \n",
    "            predFinal = predFinal > 0.5\n",
    "            correct += (output == predFinal).sum().item()\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    print(model.Boundary)\n",
    "    #print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb7a12-4536-46ad-af61-87f2597ce9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 50.1%, Avg loss: 49.981509 \n",
      "\n",
      "Parameter containing:\n",
      "tensor([-1.6805], device='cuda:0', requires_grad=True)\n",
      "Epoch 82\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = tcnPhase().to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "EPOCHS = 100000\n",
    "patience = 100\n",
    "stopped_epoch = 0\n",
    "best = np.Inf\n",
    "wait = 0\n",
    "PATH = 'Mymodel_TUG'\n",
    "\n",
    "lossHistory = []\n",
    "    \n",
    "for t in range(EPOCHS):\n",
    "    if t%10 == 0:\n",
    "        clear_output(wait=True)\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    testLoss = test(test_dataloader, model, loss_fn)\n",
    "    lossHistory.append(testLoss)\n",
    "    \n",
    "    if testLoss < best:\n",
    "        best = testLoss\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            stopped_epoch = t\n",
    "            device = torch.device(\"cuda\")\n",
    "            model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n",
    "            model.to(device)\n",
    "            break\n",
    "print(\"Done!\")\n",
    "\n",
    "plt.plot(lossHistory)\n",
    "#plt.title('InputType' + str(inputType))\n",
    "plt.tight_layout()\n",
    "#plt.savefig('InputType' + str(inputType) + '_' + root + '_LossGraph')\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dd3eb7-fcda-4522-89e8-d6ef85b96c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
